{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convert Apogee-Fire simulation data to parquet on HDFS\n",
    "\n",
    "Since there is no native reader for hdf5 data in hadoop, there is no way at the moment to efficiently deal with large files (e.g. we cannot appropriately deal with chunks and distribute them on hdfs blocks). In the Apogee-Fire case, some of the files are close to 300G each, so doing a binaryFiles read is not feasible. \n",
    "\n",
    "The method we take here is to ensure the files are accessible on the local filesystem, where POSIX semantics are used and take advantage of the chunk-based reads built into the hdf5 library. To get the data visible from executors, we need to ensure the mounts are on each Spark node, which is done by adding the following mounts on the hadoop containers in the storage-0 statefulset:\n",
    "\n",
    "NOTE: changed the mountPath from /data to /data-external as to not conflict with the hdfs path\n",
    "\n",
    "```yaml\n",
    "        - mountPath: /data-external/apogee-fire/m12f\n",
    "          name: apogee-fire-1\n",
    "        - mountPath: /data-external/apogee-fire/m12i\n",
    "          name: apogee-fire-2\n",
    "        - mountPath: /data-external/apogee-fire/m12m\n",
    "          name: apogee-fire-3\n",
    "\n",
    "```\n",
    "\n",
    "And volume definitions:\n",
    "\n",
    "```yaml\n",
    "      - name: apogee-fire-1\n",
    "        nfs:\n",
    "          path: /srv/zpool01/sdss_casload_backups/data-park/apogee-fire_sim/v1_0_1/m12f\n",
    "          server: sciserver-fs1\n",
    "      - name: apogee-fire-2\n",
    "        nfs:\n",
    "          path: /srv/zpool01/sdss_casload_backups/data-park/apogee-fire_sim/v1_0_1/m12i\n",
    "          server: sciserver-fs1\n",
    "      - name: apogee-fire-3\n",
    "        nfs:\n",
    "          path: /srv/zpool01/sdss_casload_backups/data-park/apogee-fire_sim/v1_0_1/m12m\n",
    "          server: sciserver-fs1\n",
    "\n",
    "```\n",
    "\n",
    "Instead of forcing each chunk read to go to a single task, we distribute a large-but-not-overwhelming number of tasks which means multiple chunks will land on a single partition, and as such the memory requirements per task extend beyond chunk size (but also result in more reasonable size files). The executor memory and cores here is a tradeoff, sharing 8gb between 2 tasks did not prove workable\n",
    "\n",
    "https://apps.sciserver.org/sparkgw/dracula/sparkhistory/history/application_1637717202829_0015/1/jobs/\n",
    "\n",
    "the spark.kryoserializer.buffer.max.mb can totally be tweaked.  i just sort of guessed at the number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'numExecutors': 60, 'executorCores': 1, 'executorMemory': '8gb', 'driverMemory': '2g', 'conf': {'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.kryoserializer.buffer.max.mb': '512'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"numExecutors\": 60, \"executorCores\": 1, \"executorMemory\": \"8gb\", \"driverMemory\": \"2g\", \n",
    "    \n",
    "    \"conf\": { \n",
    "       \n",
    "        \n",
    "        \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.kryoserializer.buffer.max.mb\":\"512\"\n",
    "\n",
    "        \n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>9</td><td>application_1648742864561_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://apps.sciserver.org/sparkgw/dracula/sparkhistory/history/application_1648742864561_0010\">Link</a></td><td><a target=\"_blank\" href=\"http://worker-4.worker.spark.svc.cluster.local:8042/node/containerlogs/container_e10_1648742864561_0010_01_000001/sue100\">Link</a></td><td>sue100</td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import glob\n",
    "from pyspark.sql import Row\n",
    "import healpy\n",
    "import HMpTy\n",
    "\n",
    "import astropy.units as u\n",
    "import astropy.coordinates as coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileListRdd(path):\n",
    "    files = glob.glob(f'{path}/*.hdf5')\n",
    "    return sc.parallelize(files, len(files))\n",
    "\n",
    "def getFileList(path):\n",
    "    return glob.glob(f'{path}/*.hdf5')\n",
    "\n",
    "def readH5(path):\n",
    "    return h5py.File(path)\n",
    "\n",
    "def getMaxChunkSize(h5):\n",
    "    chunk_sizes = [h5[i].chunks[0] for i in h5.keys()]\n",
    "    return max(chunk_sizes)\n",
    "\n",
    "def getDataLength(h5):\n",
    "    return h5['2MASS_magH'].shape[0]\n",
    "\n",
    "def getChunkList(h5):\n",
    "    l = getDataLength(h5)\n",
    "    cs = getMaxChunkSize(h5)\n",
    "    return zip(np.arange(0, l, cs), np.arange(0, l, cs) + cs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "#  TODO: break the obs and true stuff out\n",
    "#  into functions to remove redundant code\n",
    "####################################################\n",
    "\n",
    "def chunkToRow(h5, chunk, vctr_rot, gcen):\n",
    "    row_dict = {}\n",
    "    chunk_slice = slice(chunk[0], chunk[1])\n",
    "    for ds in h5:\n",
    "        row_dict[ds] = np.array(h5[ds][chunk_slice]).tolist()\n",
    "    \n",
    " \n",
    "    # add healpix    \n",
    "    l = np.array(h5['l'][chunk_slice])\n",
    "    b = np.array(h5['b'][chunk_slice])\n",
    "    \n",
    "    NSIDE=1048576 \n",
    "    heal20id = healpy.ang2pix(NSIDE,  l, b, nest=True, lonlat=True)\n",
    "    #theta = np.pi/2 - np.radians(dec)\n",
    "    #phi = np.radians(ra)\n",
    "    #hp8 = healpy.ang2pix(8, theta, phi, nest=True)\n",
    "    row_dict['heal20id'] = heal20id.tolist()\n",
    "    \n",
    "    \n",
    "    # add htmid \n",
    "    \n",
    "    htm20 = HMpTy.HTM(depth=20)\n",
    "    ra = np.array(h5['ra'][chunk_slice])\n",
    "    dec = np.array(h5['dec'][chunk_slice])\n",
    "    \n",
    "    htm20id = htm20.lookup_id(ra,dec)\n",
    "    row_dict['htm20id'] = htm20id.tolist()\n",
    "    \n",
    "    # add cx,cy,cz\n",
    "    cx = np.cos(np.radians(ra)) * np.cos(np.radians(dec))\n",
    "    cy = np.sin(np.radians(ra)) * np.cos(np.radians(dec))\n",
    "    cz = np.sin(np.radians(dec))\n",
    "    \n",
    "    row_dict['cx'] = cx.tolist()\n",
    "    row_dict['cy'] = cy.tolist()\n",
    "    row_dict['cz'] = cz.tolist()\n",
    "    \n",
    "    #get coords\n",
    "    #define the coordinates in ra/dec from the catalog\n",
    "    \n",
    "    parallax = np.array(h5['parallax'][chunk_slice])\n",
    "    dist = 1./parallax\n",
    "    \n",
    "        \n",
    "    parallax_true = np.array(h5['parallax_true'][chunk_slice])\n",
    "    dist_true = 1./parallax_true\n",
    "    \n",
    "    pmra  = np.array(h5['pmra'][chunk_slice])      \n",
    "    pmdec = np.array(h5['pmdec'][chunk_slice])\n",
    "    vr    = np.array(h5['radial_velocity'][chunk_slice])  #this is where the nans live!\n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    #  no NaN's in the true values!\n",
    "    \n",
    "    pmra_true = np.array(h5['pmra_true'][chunk_slice])\n",
    "    pmdec_true = np.array(h5['pmdec_true'][chunk_slice])\n",
    "    vr_true = np.array(h5['radial_velocity_true'][chunk_slice])\n",
    "    \n",
    "    ra_true = np.array(h5['ra_true'][chunk_slice])\n",
    "    dec_true = np.array(h5['dec_true'][chunk_slice])\n",
    "    \n",
    "\n",
    "    \n",
    "    c = coord.ICRS( ra=ra*u.degree, \n",
    "                dec=dec*u.degree,\n",
    "                distance=coord.Distance(dist*u.kpc, allow_negative=True),\n",
    "               \tpm_ra_cosdec=pmra*u.mas/u.yr,\n",
    "                pm_dec=pmdec*u.mas/u.yr,\n",
    "               \tradial_velocity=vr*u.km/u.s)\n",
    "    \n",
    "    c_true = coord.ICRS( ra=ra_true*u.degree, \n",
    "                dec=dec_true*u.degree,\n",
    "                distance=coord.Distance(dist_true*u.kpc, allow_negative=True),\n",
    "               \tpm_ra_cosdec=pmra_true*u.mas/u.yr,\n",
    "                pm_dec=pmdec_true*u.mas/u.yr,\n",
    "               \tradial_velocity=vr_true*u.km/u.s)\n",
    "    \n",
    "    gc = c.transform_to(gcen)\n",
    "    \n",
    "    gc_true = c_true.transform_to(gcen)\n",
    "    \n",
    "    x, y, z = gc.cartesian.xyz.value\n",
    "    x_true, y_true, z_true = gc_true.cartesian.xyz.value\n",
    "    \n",
    "    vx, vy, vz = gc.cartesian.differentials['s'].d_xyz.value\n",
    "    vx_true, vy_true, vz_true = gc_true.cartesian.differentials['s'].d_xyz.value\n",
    "    \n",
    "    row_dict['px_gal_obs'] = x.tolist()\n",
    "    row_dict['px_gal_true'] = x_true.tolist()\n",
    "    \n",
    "    row_dict['py_gal_obs'] = y.tolist()\n",
    "    row_dict['py_gal_true'] = y_true.tolist()\n",
    "    \n",
    "    row_dict['pz_gal_obs'] = z.tolist()\n",
    "    row_dict['pz_gal_true'] = z_true.tolist()\n",
    "    \n",
    "    row_dict['vx_gal_obs'] = vx.tolist()  \n",
    "    row_dict['vx_gal_true'] = vx_true.tolist()\n",
    "    \n",
    "    row_dict['vy_gal_obs'] = vy.tolist() \n",
    "    row_dict['vy_gal_true'] = vy_true.tolist() \n",
    "    \n",
    "    row_dict['vz_gal_obs'] = vz.tolist()  \n",
    "    row_dict['vz_gal_true'] = vz_true.tolist()  \n",
    "    \n",
    "    # get dgal_true\n",
    "    dhel_true = np.array(h5['dhel_true'][chunk_slice])\n",
    "    l_true = np.array(h5['l_true'][chunk_slice])\n",
    "    b_true = np.array(h5['b_true'][chunk_slice])\n",
    "    \n",
    "    dgal_true = np.sqrt(np.square(dhel_true*np.cos(np.radians(l_true))*np.cos(np.radians(b_true))-8.2)+np.square(dhel_true*np.sin(np.radians(l_true))*np.cos(np.radians(b_true)))+np.square(dhel_true*np.sin(np.radians(b_true))))\n",
    "    \n",
    "    \n",
    "    \n",
    "    row_dict['dgal_true'] = dgal_true.tolist()\n",
    "    \n",
    "    row_dict['dhel_obs'] = dist.tolist()\n",
    "    \n",
    "    # transform to cylindrical coords\n",
    "    gc.representation_type = 'cylindrical'\n",
    "    \n",
    "    rho, phi, zcyl = gc.rho.to(u.kpc).value, gc.phi.degree, gc.z.to(u.kpc).value\n",
    "    \n",
    "    VRho = gc.d_rho.to(u.km/u.s).value\n",
    "    VPhi = (gc.d_phi*gc.rho).to(u.km/u.s, equivalencies=u.dimensionless_angles()).value\n",
    "    VZcyl = gc.d_z.to(u.km/u.s).value\n",
    "   \n",
    "\n",
    "    ##############################################################################\n",
    "    # GET TRUE VERSIONS\n",
    "    ##############################################################################\n",
    "\n",
    "    gc_true.representation_type = 'cylindrical'\n",
    "    \n",
    "    rho_true, phi_true, zcyl_true = gc_true.rho.to(u.kpc).value, gc_true.phi.degree, gc_true.z.to(u.kpc).value\n",
    "    \n",
    "    VRho_true = gc_true.d_rho.to(u.km/u.s).value\n",
    "    VPhi_true = (gc_true.d_phi*gc.rho).to(u.km/u.s, equivalencies=u.dimensionless_angles()).value\n",
    "    VZcyl_true = gc_true.d_z.to(u.km/u.s).value\n",
    "    \n",
    "    \n",
    "    row_dict['rho_cyl_obs'] = rho.tolist()\n",
    "    row_dict['rho_cyl_true'] = rho_true.tolist()\n",
    "    \n",
    "    row_dict['phi_cyl_obs'] = phi.tolist()\n",
    "    row_dict['phi_cyl_true'] = phi_true.tolist()\n",
    "    \n",
    "    row_dict['z_cyl_obs'] = zcyl.tolist()\n",
    "    row_dict['z_cyl_true'] = zcyl_true.tolist()\n",
    "    \n",
    "    row_dict['vrho_cyl_obs'] = VRho.tolist()  \n",
    "    row_dict['vrho_cyl_true'] = VRho_true.tolist() \n",
    "    \n",
    "    row_dict['vphi_cyl_obs'] = VPhi.tolist()  \n",
    "    row_dict['vphi_cyl_true'] = VPhi_true.tolist() \n",
    "   \n",
    "    row_dict['vz_cyl_obs'] = VZcyl.tolist()  \n",
    "    row_dict['vz_cyl_true'] = VZcyl_true.tolist() \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return Row(**row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, DoubleType, LongType\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"2mass_magh\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magh_error\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magh_int\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magh_true\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magj\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magj_error\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magj_int\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magj_true\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magks\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magks_error\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magks_int\",FloatType(), True), \\\n",
    "    StructField(\"2mass_magks_true\",FloatType(), True), \\\n",
    "    StructField(\"a0\",FloatType(), True), \\\n",
    "    StructField(\"cfe_apogee\",FloatType(), True), \\\n",
    "    StructField(\"cafe_apogee\",FloatType(), True), \\\n",
    "    StructField(\"feh_apogee\",FloatType(), True), \\\n",
    "    StructField(\"mgfe_apogee\",FloatType(), True), \\\n",
    "    StructField(\"nfe_apogee\",FloatType(), True), \\\n",
    "    StructField(\"ofe_apogee\",FloatType(), True), \\\n",
    "    StructField(\"sfe_apogee\",FloatType(), True), \\\n",
    "    StructField(\"sife_apogee\",FloatType(), True), \\\n",
    "    StructField(\"a_g_bp_val\",FloatType(), True), \\\n",
    "    StructField(\"a_g_rp_val\",FloatType(), True), \\\n",
    "    StructField(\"a_g_val\",FloatType(), True), \\\n",
    "    StructField(\"age\",FloatType(), True), \\\n",
    "    StructField(\"alpha\",FloatType(), True), \\\n",
    "    StructField(\"b\",DoubleType(), True), \\\n",
    "    StructField(\"b_true\",DoubleType(), True), \\\n",
    "    StructField(\"bp_g\",FloatType(), True), \\\n",
    "    StructField(\"bp_g_int\",FloatType(), True), \\\n",
    "    StructField(\"bp_g_true\",FloatType(), True), \\\n",
    "    StructField(\"bp_rp\",FloatType(), True), \\\n",
    "    StructField(\"bp_rp_int\",FloatType(), True), \\\n",
    "    StructField(\"bp_rp_true\",FloatType(), True), \\\n",
    "    StructField(\"calcium\",FloatType(), True), \\\n",
    "    StructField(\"carbon\",FloatType(), True), \\\n",
    "    StructField(\"dec\",DoubleType(), True), \\\n",
    "    StructField(\"dec_error\",DoubleType(), True), \\\n",
    "    StructField(\"dec_true\",DoubleType(), True), \\\n",
    "    StructField(\"dhel_true\",DoubleType(), True), \\\n",
    "    StructField(\"dmod_true\",DoubleType(), True), \\\n",
    "    StructField(\"e_bp_min_rp_val\",FloatType(), True), \\\n",
    "    StructField(\"ebv\",FloatType(), True), \\\n",
    "    StructField(\"feh\",FloatType(), True), \\\n",
    "    StructField(\"g_rp\",FloatType(), True), \\\n",
    "    StructField(\"g_rp_int\",FloatType(), True), \\\n",
    "    StructField(\"g_rp_true\",FloatType(), True), \\\n",
    "    StructField(\"helium\",FloatType(), True), \\\n",
    "    StructField(\"l\",FloatType(), True), \\\n",
    "    StructField(\"l_true\",FloatType(), True), \\\n",
    "    StructField(\"logg\",FloatType(), True), \\\n",
    "    StructField(\"lognh\",FloatType(), True), \\\n",
    "    StructField(\"lum_val\",FloatType(), True), \\\n",
    "    StructField(\"mact\",FloatType(), True), \\\n",
    "    StructField(\"magnesium\",FloatType(), True), \\\n",
    "    StructField(\"mini\",FloatType(), True), \\\n",
    "    StructField(\"mtip\",FloatType(), True), \\\n",
    "    StructField(\"neon\",FloatType(), True), \\\n",
    "    StructField(\"nitrogen\",FloatType(), True), \\\n",
    "    StructField(\"oxygen\",FloatType(), True), \\\n",
    "    StructField(\"parallax\",DoubleType(), True), \\\n",
    "    StructField(\"parallax_error\",DoubleType(), True), \\\n",
    "    StructField(\"parallax_over_error\",FloatType(), True), \\\n",
    "    StructField(\"parallax_true\",DoubleType(), True), \\\n",
    "    StructField(\"parentid\",LongType(), True), \\\n",
    "    StructField(\"partid\",LongType(), True), \\\n",
    "    StructField(\"phot_bp_mean_mag\",FloatType(), True), \\\n",
    "    StructField(\"phot_bp_mean_mag_error\",FloatType(), True), \\\n",
    "    StructField(\"phot_bp_mean_mag_int\",FloatType(), True), \\\n",
    "    StructField(\"phot_bp_mean_mag_true\",FloatType(), True), \\\n",
    "    StructField(\"phot_g_mean_mag\",FloatType(), True), \\\n",
    "    StructField(\"phot_g_mean_mag_error\",FloatType(), True), \\\n",
    "    StructField(\"phot_g_mean_mag_int\",FloatType(), True), \\\n",
    "    StructField(\"phot_g_mean_mag_true\",FloatType(), True), \\\n",
    "    StructField(\"phot_rp_mean_mag\",FloatType(), True), \\\n",
    "    StructField(\"phot_rp_mean_mag_error\",FloatType(), True), \\\n",
    "    StructField(\"phot_rp_mean_mag_int\",FloatType(), True), \\\n",
    "    StructField(\"phot_rp_mean_mag_true\",FloatType(), True), \\\n",
    "    StructField(\"pmb_true\",DoubleType(), True), \\\n",
    "    StructField(\"pmdec\",DoubleType(), True), \\\n",
    "    StructField(\"pmdec_error\",DoubleType(), True), \\\n",
    "    StructField(\"pmdec_true\",DoubleType(), True), \\\n",
    "    StructField(\"pml_true\",DoubleType(), True), \\\n",
    "    StructField(\"pmra\",DoubleType(), True), \\\n",
    "    StructField(\"pmra_error\",DoubleType(), True), \\\n",
    "    StructField(\"pmra_true\",DoubleType(), True), \\\n",
    "    StructField(\"px_true\",DoubleType(), True), \\\n",
    "    StructField(\"py_true\",DoubleType(), True), \\\n",
    "    StructField(\"pz_true\",DoubleType(), True), \\\n",
    "    StructField(\"ra\",DoubleType(), True), \\\n",
    "    StructField(\"ra_error\",DoubleType(), True), \\\n",
    "    StructField(\"ra_true\",DoubleType(), True), \\\n",
    "    StructField(\"radial_velocity\",DoubleType(), True), \\\n",
    "    StructField(\"radial_velocity_error\",DoubleType(), True), \\\n",
    "    StructField(\"radial_velocity_true\",DoubleType(), True), \\\n",
    "    StructField(\"random_index\",LongType(), True), \\\n",
    "    StructField(\"silicon\",FloatType(), True), \\\n",
    "    StructField(\"source_id\",LongType(), True), \\\n",
    "    StructField(\"sulphur\",FloatType(), True), \\\n",
    "    StructField(\"teff_val\",FloatType(), True), \\\n",
    "    StructField(\"vx_true\",FloatType(), True), \\\n",
    "    StructField(\"vy_true\",FloatType(), True), \\\n",
    "    StructField(\"vz_true\",FloatType(), True), \\\n",
    "    StructField(\"heal20id\",LongType(), True), \\\n",
    "    StructField(\"htm20id\",LongType(), True), \\\n",
    "    StructField(\"cx\",DoubleType(), True), \\\n",
    "    StructField(\"cy\",DoubleType(), True), \\\n",
    "    StructField(\"cz\",DoubleType(), True), \\\n",
    "    #StructField(\"slice\",IntegerType(), True), \\\n",
    "    StructField(\"px_gal_obs\",DoubleType(), True), \\\n",
    "    StructField(\"px_gal_true\",DoubleType(), True), \\\n",
    "    StructField(\"py_gal_obs\",DoubleType(), True), \\\n",
    "    StructField(\"py_gal_true\",DoubleType(), True), \\\n",
    "    StructField(\"pz_gal_obs\",DoubleType(), True), \\\n",
    "    StructField(\"pz_gal_true\",DoubleType(), True), \\\n",
    "    StructField(\"vx_gal_obs\",DoubleType(), True), \\\n",
    "    StructField(\"vx_gal_true\",DoubleType(), True), \\\n",
    "    StructField(\"vy_gal_obs\",DoubleType(), True), \\\n",
    "    StructField(\"vy_gal_true\",DoubleType(), True), \\\n",
    "    StructField(\"vz_gal_obs\",DoubleType(), True), \\\n",
    "    StructField(\"vz_gal_true\",DoubleType(), True), \\\n",
    "    StructField(\"dgal_true\",DoubleType(), True), \\\n",
    "    StructField(\"dhel_obs\",DoubleType(), True), \\\n",
    "    StructField(\"rho_cyl_obs\",DoubleType(), True), \\\n",
    "    StructField(\"rho_cyl_true\",DoubleType(), True), \\\n",
    "    StructField(\"phi_cyl_obs\",DoubleType(), True), \\\n",
    "    StructField(\"phi_cyl_true\",DoubleType(), True), \\\n",
    "    StructField(\"z_cyl_obs\",DoubleType(), True), \\\n",
    "    StructField(\"z_cyl_true\",DoubleType(), True), \\\n",
    "    StructField(\"vrho_cyl_obs\",DoubleType(), True), \\\n",
    "    StructField(\"vrho_cyl_true\",DoubleType(), True), \\\n",
    "    StructField(\"vphi_cyl_obs\",DoubleType(), True), \\\n",
    "    StructField(\"vphi_cyl_true\",DoubleType(), True), \\\n",
    "    StructField(\"vz_cyl_obs\",DoubleType(), True), \\\n",
    "    StructField(\"vz_cyl_true\",DoubleType(), True) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m12f-lsr0\n",
      "/data-external/apogee-fire/m12f/lsr_0\n",
      "hdfs:///data/apogee_fire/m12f/lsr_0\n",
      "root\n",
      " |-- Label: string (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: integer (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- v_R_LSR: double (nullable = true)\n",
      " |-- v_Z_LSR: double (nullable = true)\n",
      " |-- v_phi_LSR: double (nullable = true)\n",
      "\n",
      "+---------+-------+----+---+---------+---------+-------+-------+-------+---------+\n",
      "|    Label|     px|  py| pz|       vx|       vy|     vz|v_R_LSR|v_Z_LSR|v_phi_LSR|\n",
      "+---------+-------+----+---+---------+---------+-------+-------+-------+---------+\n",
      "|m12i-lsr0|    0.0| 8.2|  0| 224.7092| -20.3801| 3.8954|  -17.8|   -3.9|    224.4|\n",
      "|m12i-lsr1|-7.1014|-4.1|  0| -80.4269|  191.724| 1.5039|  -24.4|   -1.5|    210.9|\n",
      "|m12i-lsr2| 7.1014|-4.1|  0| -87.2735|-186.8567|-9.4608|   22.1|    9.5|    206.5|\n",
      "|m12f-lsr0|    0.0| 8.2|  0| 226.1849|  14.3773|-4.8906|   14.9|    4.9|    227.9|\n",
      "|m12f-lsr1|-7.1014|-4.1|  0|-114.0351| 208.7267| 5.0635|   -3.4|   -5.1|    244.3|\n",
      "|m12f-lsr2| 7.1014|-4.1|  0| -118.143|-187.7631|-3.8905|  -11.4|    3.9|    227.4|\n",
      "|m12m-lsr0|    0.0| 8.2|  0| 254.9187|  16.7901| 1.9648|   16.2|   -2.0|    254.7|\n",
      "|m12m-lsr1|-7.1014|-4.1|  0| -128.248| 221.1489| 5.8506|    2.4|   -5.9|    252.7|\n",
      "|m12m-lsr2| 7.1014|-4.1|  0|-106.6203|-232.2056|-6.4185|   15.4|    6.4|    265.3|\n",
      "+---------+-------+----+---+---------+---------+-------+-------+-------+---------+"
     ]
    }
   ],
   "source": [
    "\n",
    "# i know this sucks, leave me alone\n",
    "sim = 'm12f'\n",
    "lsr =  'lsr_0' #lsr_n or test, tests are based on lsr0 so...\n",
    "nlsr = lsr\n",
    "if (lsr == 'test'):\n",
    "    nlsr = 'lsr_0'\n",
    "label = f'{sim}-{nlsr}'.replace('_','')\n",
    "print(label)\n",
    "\n",
    "\n",
    "path = f'/data-external/apogee-fire/{sim}/{lsr}'\n",
    "hdfs_path = f'hdfs:///data/apogee_fire/{sim}/{lsr}'\n",
    "\n",
    "\n",
    "\n",
    "print(path)\n",
    "print(hdfs_path)\n",
    "\n",
    "csv_path = 'hdfs:///data/apogee_fire/sandersont4.csv'\n",
    "csv_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
    "csv_df.printSchema()\n",
    "csv_df.show()\n",
    "\n",
    "#path = '/data-external/apogee-fire/m12f/lsr_0'\n",
    "#hdfs_path = 'hdfs:///data/apogee_fire/m12f/lsr_0'\n",
    "\n",
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# JUST FOR THIS TEST\n",
    "#hdfs_path = 'hdfs:///data/apogee_fire/m12f/test_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Galactocentric Frame (galcen_coord=<ICRS Coordinate: (ra, dec) in deg\n",
      "    (266.4051, -28.936175)>, galcen_distance=8.2 kpc, galcen_v_sun=(-14.3773, 226.1849, -4.8906) km / s, z_sun=0.0 pc, roll=0.0 deg)>"
     ]
    }
   ],
   "source": [
    "\n",
    "# this stuff doesn't need catalog data i can just do it once per survey here\n",
    "# if i were smart i would serialize this to disk or something but i'm not smart so\n",
    "\n",
    "\n",
    "xvsun = np.array(csv_df.select(\"px\",\"py\",\"pz\",\"vx\",\"vy\",\"vz\").filter(csv_df.Label == label).collect())\n",
    "xvsun[0]  # i actually don't know why this is a 2d array but whatever who cares\n",
    "          # oh maybe it's because of the \"filter\" thing, it doesn't know it will just get one row\n",
    "\n",
    "# thought i could do it all at once, it didn't work\n",
    "#xvsun = np.array(df.columns[4:]).collect()\n",
    "\n",
    "\n",
    "\n",
    "phi = np.pi + np.arctan2(xvsun[0][1], xvsun[0][0])\n",
    "phi\n",
    "\n",
    "rot = np.array([\n",
    "    [np.cos(phi), np.sin(phi), 0.0],\n",
    "    [-np.sin(phi), np.cos(phi), 0.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "    ])\n",
    "\n",
    "vctr_rot = np.dot(rot, xvsun[0][3:])\n",
    "vctr_rot\n",
    "\n",
    "\n",
    "gcen = coord.Galactocentric(galcen_distance=8.2*u.kpc, z_sun=0.0*u.kpc,galcen_v_sun=coord.CartesianDifferential(vctr_rot*u.km/u.s))\n",
    "gcen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>setprog(93.21417483478956)</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "write = True\n",
    "\n",
    "files = getFileList(path)\n",
    "files.sort()\n",
    "for index, file in enumerate(files, start=0):\n",
    "    out_path = f'{hdfs_path}/slice={index}'\n",
    "    print(out_path)\n",
    "    #if (fs.exists(sc._jvm.org.apache.hadoop.fs.Path(f'{out_path}/_SUCCESS'))) == False:\n",
    "    #if (index == 0):   \n",
    "    if (True == True):\n",
    "    \n",
    "        ds = sc.parallelize([file]).map(\n",
    "            lambda x: (x, readH5(x))\n",
    "        ).flatMapValues(\n",
    "            getChunkList\n",
    "        ).map(\n",
    "            lambda x: (f'{x[0]}-{x[1][0]}', (x[0], x[1]))\n",
    "        ).partitionBy(\n",
    "            2500\n",
    "        ).map(\n",
    "            lambda x: chunkToRow(readH5(x[1][0]), x[1][1], vctr_rot, gcen)\n",
    "        )\n",
    "    \n",
    "        df = spark.createDataFrame(ds)\n",
    "        df.createOrReplaceTempView('arrRows')\n",
    "        #df.printSchema()\n",
    "        \n",
    "        out_df = spark.sql('''\n",
    "            SELECT a.* FROM (\n",
    "                SELECT explode(arrays_zip(*)) a FROM  arrRows\n",
    "            )\n",
    "            ''')\n",
    "        \n",
    "        #maybe this will work?\n",
    "        #use schema defined above to separate floats / doubles\n",
    "        out_df = spark.createDataFrame(out_df.rdd, schema)\n",
    "        #out_df.printSchema()\n",
    "        \n",
    "        cols = [i.name for i in out_df.schema]\n",
    "        cols_renamed = [i.lower().replace('-', '_') for i in cols]\n",
    "        #cols_renamed\n",
    "        \n",
    "        for pair in zip(cols, cols_renamed):\n",
    "            out_df = out_df.withColumnRenamed(*pair)\n",
    "            \n",
    "        if (write == True):\n",
    "        \n",
    "            sc.setJobDescription(f'write to parquet {out_path}')\n",
    "        \n",
    "            #out_df.show(10)\n",
    "\n",
    "            out_df.write.mode('overwrite').parquet(out_path)\n",
    "        else:\n",
    "            out_df.show(10)\n",
    "    else:\n",
    "        print(f'{out_path} exists, skipping...')\n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.printSchema()\n",
    "out_df.createOrReplaceTempView('outdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+----------+----------+----------+------------------+------------------+------------------+-------------------+--------------------+------------+------------+----------+\n",
      "|         px_gal_obs|         py_gal_obs|          pz_gal_obs|vx_gal_obs|vy_gal_obs|vz_gal_obs|         dgal_true|         dhel_true|       rho_cyl_obs|        phi_cyl_obs|           z_cyl_obs|vrho_cyl_obs|vphi_cyl_obs|vz_cyl_obs|\n",
      "+-------------------+-------------------+--------------------+----------+----------+----------+------------------+------------------+------------------+-------------------+--------------------+------------+------------+----------+\n",
      "| -7.833710941746277| 0.5096609785983679|  0.1053370875582952|       NaN|       NaN|       NaN|10.381756353937098|12.645137661316038| 7.850272697934843|    176.27758461593|  0.1053370875582952|         NaN|         NaN|       NaN|\n",
      "|  13.87824558613573| 32.416141173246785|   4.074440887625832|       NaN|       NaN|       NaN|10.789679994603066|12.973211896491891| 35.26204629786765|  66.82289519355923|   4.074440887625832|         NaN|         NaN|       NaN|\n",
      "|-11.084173051875107|-3.9242483193448834| -0.5686071911603385|       NaN|       NaN|       NaN|10.549939612296974|13.028680428771233|11.758342447632467|  -160.503965010179| -0.5686071911603385|         NaN|         NaN|       NaN|\n",
      "| -3.970903556067917|  5.635798152840811|  1.1246256559108136|       NaN|       NaN|       NaN|10.075864722552918|12.469628693389954| 6.894221919198477| 125.16798221189556|  1.1246256559108136|         NaN|         NaN|       NaN|\n",
      "| -8.552007337486174|-0.5010065467937987|-0.08146110030010846|       NaN|       NaN|       NaN|10.565228504711454|12.810494122145617| 8.566670126738137|-176.64724461189263|-0.08146110030010846|         NaN|         NaN|       NaN|\n",
      "| -36.38995472767478| -35.14449089382454|  -4.884753593308516|       NaN|       NaN|       NaN| 9.673498637889786|12.336542432281071|50.590157592839496|-135.99745809763172|  -4.884753593308516|         NaN|         NaN|       NaN|\n",
      "|  -7.30825599416143| 1.2656064717180728| 0.18982814293801492|       NaN|       NaN|       NaN|10.222224036209326|12.384090874526969| 7.417032116517437|   170.175249279361| 0.18982814293801492|         NaN|         NaN|       NaN|\n",
      "| -8.597125380878037|-0.5306634573388699|-0.09080922340409105|       NaN|       NaN|       NaN|10.040632027382646|12.435173938685633| 8.613487593274408|-176.46786007411322|-0.09080922340409105|         NaN|         NaN|       NaN|\n",
      "| -66.69187935213196| -74.65813421926401|  -9.159522237608261|       NaN|       NaN|       NaN| 9.845223518401342|12.451725362286892|100.10816038975527|-131.77431618628535|  -9.159522237608261|         NaN|         NaN|       NaN|\n",
      "|-14.599661760980752| -8.763624800959931| -1.0939361484067887|       NaN|       NaN|       NaN|10.058612013123676|12.367065972196578|17.027954756430486| -149.0251992842733| -1.0939361484067887|         NaN|         NaN|       NaN|\n",
      "+-------------------+-------------------+--------------------+----------+----------+----------+------------------+------------------+------------------+-------------------+--------------------+------------+------------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "mydf = spark.sql('''select px_gal_obs, py_gal_obs, pz_gal_obs, \n",
    "                 vx_gal_obs, vy_gal_obs, vz_gal_obs, \n",
    "                 dgal_true, dhel_true, \n",
    "                 rho_cyl_obs, phi_cyl_obs, z_cyl_obs, \n",
    "                 vrho_cyl_obs, vphi_cyl_obs, vz_cyl_obs \\\n",
    "                 from outdf''')\n",
    "\n",
    "mydf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok now we have out_df which is the stuff ready to be written to parquet.  should we do the extra stuff before we write to parquet or after?\n",
    " - change column names\n",
    " - get rid of NaN's (can do that at ingest time)\n",
    " - add healpix (maybe we should do this as a separate step)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ingest stuff down here\n",
    "\n",
    "should move to different notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hdfs_path = 'hdfs:///data/apogee_fire/m12f/lsr_0/'\n",
    "testDF = spark.read.parquet(hdfs_path)\n",
    "testDF.createOrReplaceTempView('testDF')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135"
     ]
    }
   ],
   "source": [
    "len(testDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|radial_velocity|radial_velocity_true|\n",
      "+---------------+--------------------+\n",
      "|            NaN|   -128.853320708274|\n",
      "|            NaN| -45.082068589629046|\n",
      "|            NaN|  -42.79229241661227|\n",
      "|            NaN|   4.565146946236344|\n",
      "|            NaN| -4.1852082420782954|\n",
      "|            NaN|  -6.307093176029351|\n",
      "|            NaN|    1.78165005474733|\n",
      "|            NaN| -0.8437668269285681|\n",
      "|            NaN| -14.762100855508505|\n",
      "|            NaN|   8.546889179502031|\n",
      "+---------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mydf = spark.sql('''select px_gal_obs, py_gal_obs, pz_gal_obs, \n",
    "                 vx_gal_obs, vy_gal_obs, vz_gal_obs, \n",
    "                 dgal_true, dhel_true, \n",
    "                 rho_cyl_obs, phi_cyl_obs, z_cyl_obs, \n",
    "                 vrho_cyl_obs, vphi_cyl_obs, vz_cyl_obs \\\n",
    "                 from testDF''')\n",
    "\n",
    "\n",
    "\n",
    "my_df = spark.sql('''select radial_velocity, radial_velocity_true from testDF''')\n",
    "my_df.show(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "columns = testDF.columns\n",
    "for column in columns:\n",
    "    testDF = testDF.withColumn(column,F.when(F.isnan(F.col(column)),F.lit(-9999)).otherwise(F.col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>idle()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_name = \"jdbc:sqlserver://sdss4c:1433\"\n",
    "#database_name= \"fire_m12f_lsr2\"\n",
    "database_name = \"apogee_fire_test\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"m12f_lsr0\"\n",
    "username = \"connector_user\"\n",
    "password = \"password123!#\" # Please specify password here\n",
    "\n",
    "try:\n",
    "  testDF.write \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"truncate\",\"true\") \\\n",
    "    .option(\"reliabilityLevel\",\"BEST_EFFORT\") \\\n",
    "    .option(\"tableLock\",\"false\") \\\n",
    "    .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"batchsize\", \"1048576\") \\\n",
    "    .save()\n",
    "except ValueError as error :\n",
    "    print(\"Connector write failed\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "sciserver": {
   "copySource": {
    "path": "ztf/ztf-clustering-partitioned-nn",
    "volId": "49850",
    "volType": "uservolumes"
   },
   "imageInfo": {
    "cachedContainer": {
     "arik": 109855
    },
    "dataVolumes": [],
    "domain": 6,
    "name": "Dracula Spark",
    "userVolumes": [
     49850,
     49851
    ]
   },
   "lastEdit": {
    "time": 1635474064301,
    "user": "arik"
   },
   "notebookId": "YXJpazE2MzQwNjU3OTc0NzI="
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "47.6px",
    "left": "1237px",
    "top": "107.8px",
    "width": "159px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
