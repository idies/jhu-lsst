{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the spark session. These files are small, so we can fit more tasks onto executors. Seems like 4 cores / 4GB works. Total of 120 cores means we can fit 29 (since we need a core for the driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1588740809550_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1588740809550_0006/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-1.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1588740809550_0006_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'arik-load-logcube-write', 'executorMemory': '4G', 'numExecutors': 29, 'executorCores': 4, 'conf': {'spark.yarn.appMasterEnv.PYSPARK_PYTHON': 'python3'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1588740809550_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1588740809550_0006/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-1.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1588740809550_0006_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"arik-load-logcube-write\", \"executorMemory\": \"4G\", \"numExecutors\": 29, \"executorCores\": 4,\n",
    " \"conf\": {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\":\"python3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from gzip import GzipFile\n",
    "from pyspark.sql import Row\n",
    "import glob\n",
    "import time\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_fits_module():\n",
    "    if 'astropy' not in sys.modules:\n",
    "        stdout = subprocess.check_output(\n",
    "            sys.executable + ' -m pip install astropy',\n",
    "            stderr=subprocess.STDOUT,\n",
    "            shell=True).decode('utf-8')\n",
    "    from astropy.io import fits\n",
    "    return fits\n",
    "\n",
    "def get_wcs_module():\n",
    "    if 'astropy' not in sys.modules:\n",
    "        stdout = subprocess.check_output(\n",
    "            sys.executable + ' -m pip install astropy',\n",
    "            stderr=subprocess.STDOUT,\n",
    "            shell=True).decode('utf-8')\n",
    "    from astropy import wcs\n",
    "    return wcs\n",
    "\n",
    "def getfitslocal(path):\n",
    "    fits = get_fits_module()\n",
    "    fits_obj = fits.open(path)\n",
    "    return fits_obj\n",
    "\n",
    "# Spark dataframe cannot do numpy types\n",
    "def typeconv(i):\n",
    "    try:\n",
    "        return i.item()\n",
    "    except:\n",
    "        return i\n",
    "\n",
    "def createSpaxelMapRows(fits):\n",
    "    # Scan through X, Y pixel space of flux cube, discard empty spectra\n",
    "    rows = []\n",
    "    n_y, n_x = fits['SPX_SNR'].data.shape\n",
    "    # prepare data for EMLINE fields\n",
    "    hdu = fits['EMLINE_SFLUX']\n",
    "    emlines = [hdu.header['C{:02d}'.format(i+1)] for i in range(hdu.data.shape[0])]\n",
    "    emline_hdus = [i.name for i in fits if i.name.startswith('EMLINE_')]\n",
    "    # Stellar\n",
    "    stellar_hdus = [i.name for i in fits if i.name.startswith('STELLAR_') and i.name!='STELLAR_CONT_FRESID']\n",
    "    # flat image dimension hdus\n",
    "    flat_hdus = ['SPX_MFLUX', 'SPX_MFLUX_IVAR', 'SPX_SNR', \n",
    "                 'BIN_AREA', 'BIN_FAREA', 'BIN_MFLUX', 'BIN_MFLUX_IVAR', 'BIN_MFLUX_MASK', 'BIN_SNR'] + \\\n",
    "        stellar_hdus\n",
    "    # prepare coordinate info\n",
    "    objra, objdec = typeconv(fits[0].header['OBJRA']), typeconv(fits[0].header['OBJDEC'])\n",
    "    # these headers are specific to file, so collect them in appropriate format before loop\n",
    "    per_file_data = [(i, typeconv(fits[0].header[i])) for i in ['PLATEID', 'IFUDSGN', 'MJDMED']]\n",
    "    for x in range(n_x):\n",
    "        for y in range(n_y):\n",
    "            # Skip if the spaxel is not populated, using g-band mean flux as proxy\n",
    "            if fits['SPX_MFLUX'].data[y, x] == 0:\n",
    "                continue\n",
    "            base = [(hdu, typeconv(fits[hdu].data[y, x])) for hdu in flat_hdus] + [ \\\n",
    "                ('RA', objra + typeconv(fits['SPX_SKYCOO'].data[0, y, x])),\n",
    "                ('DEC', objdec + typeconv(fits['SPX_SKYCOO'].data[1, y, x])),\n",
    "                ('GAL_R', typeconv(fits['SPX_ELLCOO'].data[0, y, x])),\n",
    "                ('GAL_R_REFF', typeconv(fits['SPX_ELLCOO'].data[1, y, x])),\n",
    "                ('GAL_THETA', typeconv(fits['SPX_ELLCOO'].data[2, y, x])),\n",
    "                ('STELLAR_CONT_FRESID_68', typeconv(fits['STELLAR_CONT_FRESID'].data[0, y, x])),\n",
    "                ('STELLAR_CONT_FRESID_99', typeconv(fits['STELLAR_CONT_FRESID'].data[0, y, x])),\n",
    "                ('BIN_LWSKYCOO_RA', typeconv(fits['BIN_LWSKYCOO'].data[0, y, x])),\n",
    "                ('BIN_LWSKYCOO_DEC', typeconv(fits['BIN_LWSKYCOO'].data[1, y, x])),    \n",
    "                ('BIN_LWELLCOO_GAL_R', typeconv(fits['BIN_LWELLCOO'].data[0, y, x])),\n",
    "                ('BIN_LWELLCOO_GAL_R_REFF', typeconv(fits['BIN_LWELLCOO'].data[1, y, x])),\n",
    "                ('BIN_LWELLCOO_GAL_THETA', typeconv(fits['BIN_LWELLCOO'].data[2, y, x])),\n",
    "            ]\n",
    "            for l in range(len(emlines)):\n",
    "                row = [('LINE', emlines[l]), ('_SRC_X', x), ('_SRC_Y', y)] + \\\n",
    "                    base + \\\n",
    "                    per_file_data + \\\n",
    "                    [(hdu, typeconv(fits[hdu].data[l, y, x])) for hdu in emline_hdus]\n",
    "                rows.append(Row(**dict(row)))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read local fits to parquet\n",
    "Here we scan all directories that have a stack subdir and identify all LOGRSS files within them. Each file will be given it's own partition, so first we scan for the total dataset size including number of files, sum of filesize and max filesize (this will determine how much memory tasks need)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_dir = '/sciserver/vc/manga/vc/sas/dr15/manga/spectro/analysis/v2_4_3/2.2.1/VOR10-GAU-MILESHC/'\n",
    "dirs1 = [base_dir + i for i in os.listdir(base_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files_rdd = sc.parallelize(dirs1, len(dirs1)).flatMap(lambda x: [x+'/'+i for i in os.listdir(x)]).flatMap(lambda d: glob.glob(d+'/*-MAPS-*.fits.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N files: 4722. Total Size: 3736MB, average size: 4"
     ]
    }
   ],
   "source": [
    "files_stats = files_rdd.map(\n",
    "    lambda x: (os.stat(x).st_size/1024/1024, 1, os.stat(x).st_size/1024/1024)\n",
    ").reduce(\n",
    "    lambda x,y: (x[0]+y[0], x[1]+y[1], max(x[2],y[2]))\n",
    ")\n",
    "print('N files: {1}. Total Size: {0:0.0f}MB, average size: {2:0.0f}'.format(*files_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_part = int(files_stats[1])\n",
    "table_data = files_rdd.repartition(n_part).map(getfitslocal).flatMap(createSpaxelMapRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "This is a time consuming job. The memory requirements are not so high (can fit in 1G per task), but the data conversion throughput is very low - so takes a while (16hrs / ~12sec per file on average). The dataset size remains about the same ~4G in fits and in this parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hdfs_dir = 'hdfs:///manga/arik-test/dr15/v2_4_3/maps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61066.266315698624"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "table = spark.createDataFrame(table_data)\n",
    "table.write.mode('overwrite').parquet(hdfs_dir)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.parquet(hdfs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|182235416|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''select count(*) from data''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-----------+\n",
      "|plateid|ifudsgn|count(1)|min(_src_x)|max(_src_x)|\n",
      "+-------+-------+--------+-----------+-----------+\n",
      "|   8084|  12704|   68970|          5|         72|\n",
      "|   8993|   3702|   19712|          4|         40|\n",
      "|   9049|   3703|   19492|          3|         38|\n",
      "|   8440|   6104|   32054|          5|         51|\n",
      "|   8552|   9101|   47036|          6|         61|\n",
      "|   9881|   3702|   20482|          4|         41|\n",
      "|   8085|  12703|   65758|          5|         69|\n",
      "|   8486|   3702|   20416|          5|         41|\n",
      "|   8448|   1901|   10692|          5|         30|\n",
      "|   8261|   3701|   19646|          5|         40|\n",
      "|   8724|   9101|   47938|          4|         60|\n",
      "|   8985|   3701|   19976|          5|         41|\n",
      "|   7443|  12701|   61996|          4|         68|\n",
      "|   8262|  12701|   65736|          6|         71|\n",
      "|   8315|   3701|   19778|          6|         41|\n",
      "|   7964|  12705|   68354|          5|         71|\n",
      "|   9507|   9101|   47454|          4|         60|\n",
      "|   8449|   3704|   19470|          4|         40|\n",
      "|   9497|  12704|   67408|          4|         71|\n",
      "|   8323|   3702|   19932|          6|         42|\n",
      "+-------+-------+--------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "4.3866565227508545"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "spark.sql('''\n",
    "SELECT plateid, ifudsgn, count(*), min(_src_x), max(_src_x) FROM data GROUP BY plateid, ifudsgn\n",
    "''').show()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|124228735|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT count(*) FROM data WHERE EMLINE_SFLUX!=0''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So some portion of lines have no flux (even when they have some g-band total flux). That portion is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.16936999776158"
     ]
    }
   ],
   "source": [
    "124228735/182235416*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
