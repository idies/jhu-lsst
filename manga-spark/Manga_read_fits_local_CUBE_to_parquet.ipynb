{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the spark session. Switch to something like 26G, 8 exec, 4 cores per for prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'arik-load-logrss', 'executorMemory': '26G', 'numExecutors': 8, 'executorCores': 2, 'conf': {'spark.yarn.appMasterEnv.PYSPARK_PYTHON': 'python3'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"arik-load-logrss\", \"executorMemory\": \"26G\", \"numExecutors\": 8, \"executorCores\": 2,\n",
    " \"conf\": {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\":\"python3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>37</td><td>application_1586890731024_0036</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1586890731024_0036/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-1.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1586890731024_0036_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from gzip import GzipFile\n",
    "from pyspark.sql import Row\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_fits_module():\n",
    "    if 'astropy' not in sys.modules:\n",
    "        stdout = subprocess.check_output(\n",
    "            sys.executable + ' -m pip install astropy',\n",
    "            stderr=subprocess.STDOUT,\n",
    "            shell=True).decode('utf-8')\n",
    "    from astropy.io import fits\n",
    "    return fits\n",
    "\n",
    "def get_wcs_module():\n",
    "    if 'astropy' not in sys.modules:\n",
    "        stdout = subprocess.check_output(\n",
    "            sys.executable + ' -m pip install astropy',\n",
    "            stderr=subprocess.STDOUT,\n",
    "            shell=True).decode('utf-8')\n",
    "    from astropy import wcs\n",
    "    return wcs\n",
    "\n",
    "def getfitslocal(path):\n",
    "    fits = get_fits_module()\n",
    "    fits_obj = fits.open(path)\n",
    "    return fits_obj\n",
    "\n",
    "# Spark dataframe cannot do numpy types\n",
    "def typeconv(i):\n",
    "    try:\n",
    "        return i.item()\n",
    "    except:\n",
    "        return i\n",
    "\n",
    "def createSpaxelSpectrumRows(fits):\n",
    "    # Scan through X, Y pixel space of flux cube, discard empty spectra\n",
    "    wcs = get_wcs_module()\n",
    "    wc = wcs.WCS(fits['FLUX'].header)\n",
    "    n_wav, n_y, n_x = fits['FLUX'].data.shape\n",
    "    rows = []\n",
    "    for x in range(n_x):\n",
    "        for y in range(n_y):\n",
    "            spec = fits['FLUX'].data[:, y, x]\n",
    "            if spec.sum() == 0:\n",
    "                continue\n",
    "            ra, dec, _ = wc.all_pix2world([[x, y, 0]], 0)[0]\n",
    "            row = {\n",
    "                'RA': typeconv(ra),\n",
    "                'DEC': typeconv(dec),\n",
    "                '_SRC_X': x,\n",
    "                '_SRC_Y': y,\n",
    "            }\n",
    "            # metadata, could be more relevant items, but for obsinfo I think we need to refer to \n",
    "            # another table anyway, so we need uniquely identifying items here, and might also just \n",
    "            # keep a particularly set of handy things (e.g. frequently queried)\n",
    "            for h in ['PLATEID', 'IFUDSGN', 'DESIGNID', 'MANGAID', 'MJDRED', 'DATE-OBS']:\n",
    "                row[h] = typeconv(fits['FLUX'].header[h])\n",
    "            # Cube units\n",
    "            for unit in ['FLUX', 'IVAR', 'MASK', 'DISP', 'PREDISP']:\n",
    "                row[unit] = fits[unit].data[:, y, x].tolist()\n",
    "            # Spectral axis units\n",
    "            for unit in ['WAVE', 'SPECRES', 'SPECRESD', 'PRESPECRES', 'PRESPECRESD']:\n",
    "                row[unit] = fits[unit].data.tolist()\n",
    "            # broadband images, one value per spaxel\n",
    "            for unit in ['G', 'R', 'I', 'Z']:\n",
    "                row[unit] = fits[unit + 'IMG'].data[y, x].tolist()\n",
    "            rows.append(Row(**row))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read local fits to parquet\n",
    "Here we scan all directories that have a stack subdir and identify all LOGRSS files within them. Each file will be given it's own partition, so first we scan for the total dataset size including number of files, sum of filesize and max filesize (this will determine how much memory tasks need)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_dir = '/sciserver/vc/manga/vc/sas/dr15/manga/spectro/redux/v2_4_3/'\n",
    "stack_dir = lambda x: base_dir + x + '/stack'\n",
    "rss_dirs = [stack_dir(i) for i in\n",
    "            os.listdir(base_dir)\n",
    "            if os.path.isdir(stack_dir(i))\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files_rdd = sc.parallelize(rss_dirs).flatMap(lambda d: glob.glob(d+'/*-LOGCUBE.fits.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N files: 4857. Total Size: 597114MB, average size: 214"
     ]
    }
   ],
   "source": [
    "files_stats = files_rdd.map(\n",
    "    lambda x: (os.stat(x).st_size/1024/1024, 1, os.stat(x).st_size/1024/1024)\n",
    ").reduce(\n",
    "    lambda x,y: (x[0]+y[0], x[1]+y[1], max(x[2],y[2]))\n",
    ")\n",
    "print('N files: {1}. Total Size: {0:0.0f}MB, average size: {2:0.0f}'.format(*files_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_part = int(files_stats[1])\n",
    "table_data = files_rdd.repartition(n_part).map(getfitslocal).flatMap(createSpaxelSpectrumRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "This is a big job. In this data format we end up blowing up the size due to some repeated columns that do not benefit well from RLE (for example, the array columns). It ends up creating about 1.3TB of parquet files, which are much larger in memory and with column format this needs to be stored in RAM of tasks. If there are problems, it is quite likely not enough memory is allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22478.775051355362"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "hdfs_dir = 'hdfs:///manga/arik-test/dr15/v2_4_3/logcube'\n",
    "table = spark.createDataFrame(table_data)\n",
    "table.write.mode('overwrite').parquet(hdfs_dir)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
