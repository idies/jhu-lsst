{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'arik-2', 'executorMemory': '26G', 'numExecutors': 8, 'executorCores': 4, 'conf': {'spark.yarn.appMasterEnv.PYSPARK_PYTHON': 'python3'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"arik-load-logrss\", \"executorMemory\": \"26G\", \"numExecutors\": 8, \"executorCores\": 4,\n",
    " \"conf\": {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\":\"python3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>169</td><td>application_1580142637008_0175</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1580142637008_0175/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-0.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1580142637008_0175_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from gzip import GzipFile\n",
    "from pyspark.sql import Row\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_fits_module():\n",
    "    if 'astropy' not in sys.modules:\n",
    "        stdout = subprocess.check_output(\n",
    "            sys.executable + ' -m pip install astropy',\n",
    "            stderr=subprocess.STDOUT,\n",
    "            shell=True).decode('utf-8')\n",
    "    from astropy.io import fits\n",
    "    return fits\n",
    "\n",
    "def getfits(compressed_data):\n",
    "    fits = get_fits_module()\n",
    "    decomp = GzipFile(fileobj=BytesIO(compressed_data)).read()\n",
    "    fits_obj = fits.open(BytesIO(decomp))\n",
    "    return fits_obj\n",
    "\n",
    "def getfitslocal(path):\n",
    "    fits = get_fits_module()\n",
    "    fits_obj = fits.open(path)\n",
    "    return fits_obj\n",
    "\n",
    "# Spark dataframe cannot do numpy types\n",
    "def typeconv(i):\n",
    "    try:\n",
    "        return i.item()\n",
    "    except:\n",
    "        return i\n",
    "\n",
    "def headerDict(fits):\n",
    "    return dict(fits[0].header.items())\n",
    "\n",
    "def createFiberExposureRows(fits):\n",
    "    obsinfo = fits['OBSINFO']\n",
    "    obsinfo_columns = [i.name for i in obsinfo.columns]\n",
    "    n_exposures = len(obsinfo.data)\n",
    "    n_fibers = int(len(fits['FLUX'].data)/n_exposures)\n",
    "    rows = []\n",
    "    for exp in range(n_exposures):\n",
    "        expinfo = dict(zip(obsinfo_columns, [typeconv(i) for i in obsinfo.data[exp]]))\n",
    "        for fiber in range(n_fibers):\n",
    "            ind = exp*n_fibers + fiber\n",
    "            row = headerDict(fits)\n",
    "            row.update(expinfo)\n",
    "            row['EXPOSURE_INDEX'] = exp\n",
    "            row['FIBER_INDEX'] = fiber\n",
    "            # Per fiber/exposure data\n",
    "            for unit in ['FLUX', 'XPOS', 'YPOS', 'IVAR', 'MASK', 'DISP']:\n",
    "                row[unit] = fits[unit].data[ind].tolist()\n",
    "            # references all spectra\n",
    "            for unit in ['WAVE', 'SPECRES', 'SPECRESD']:\n",
    "                row[unit] = fits[unit].data.tolist()\n",
    "            # for convenience, store the mean fiber positions\n",
    "            row['XPOS_MEAN'] = fits['XPOS'].data[ind].mean().tolist()\n",
    "            row['YPOS_MEAN'] = fits['YPOS'].data[ind].mean().tolist()\n",
    "            rows.append(Row(**row))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read local fits to parquet\n",
    "Here we scan all directories that have a stack subdir and identify all LOGRSS files within them. Each file will be given it's own partition, so first we scan for the total dataset size including number of files, sum of filesize and max filesize (this will determine how much memory tasks need)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_dir = '/sciserver/vc/manga/vc/sas/dr15/manga/spectro/redux/v2_4_3/'\n",
    "stack_dir = lambda x: base_dir + x + '/stack'\n",
    "rss_dirs = [stack_dir(i) for i in\n",
    "            os.listdir(base_dir)\n",
    "            if os.path.isdir(stack_dir(i))\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rss_files_rdd = sc.parallelize(rss_dirs).flatMap(lambda d: glob.glob(d+'/*-LOGRSS.fits.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319175.72129917145, 4857, 197.0564022064209)"
     ]
    }
   ],
   "source": [
    "files_stats = rss_files_rdd.map(\n",
    "    lambda x: (os.stat(x).st_size/1024/1024, 1, os.stat(x).st_size/1024/1024)\n",
    ").reduce(\n",
    "    lambda x,y: (x[0]+y[0], x[1]+y[1], max(x[2],y[2]))\n",
    ")\n",
    "files_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_part = int(files_stats[1])\n",
    "fiber_exposures = rss_files_rdd.repartition(n_part).map(getfitslocal).flatMap(createFiberExposureRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "This is a big job. It ends up creating a bit over 500GB of parquet files, which are much larger in memory and with column format this needs to be stored in RAM of tasks. If there are problems, it is quite likely not enough memory is allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df2042ae964af4a9ffc305b4dbd469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time.time()\n",
    "hdfs_dir = 'hdfs:///manga/arik-test/dr15/v2_4_3/logrss'\n",
    "table = spark.createDataFrame(fiber_exposures)\n",
    "table.write.mode('overwrite').parquet(hdfs_dir)\n",
    "print(time.time()-t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
