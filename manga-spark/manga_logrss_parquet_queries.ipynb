{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'arik-manga-query', 'executorMemory': '12G', 'numExecutors': 16, 'executorCores': 4, 'conf': {'spark.yarn.appMasterEnv.PYSPARK_PYTHON': 'python3'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"arik-manga-query\", \"executorMemory\": \"12G\", \"numExecutors\": 16, \"executorCores\": 4,\n",
    " \"conf\": {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\":\"python3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>171</td><td>application_1580142637008_0177</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1580142637008_0177/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-0.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1580142637008_0177_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "hdfs_dir = 'hdfs:///manga/arik-test/dr15/v2_4_3/logrss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table setup\n",
    "In each of the queries below, we are reading the dataset from the parquet files, as the size is too large for our cluster. The OS will likely as much as possible of what we read anyhow, but will certainly be smaller than the largest queries and disk reads are guaranteed. We benefit from large blocks/files and locality in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.636242151260376"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "manga = spark.read.parquet(hdfs_dir)# .cache(), data size is too large for cluster to cache\n",
    "manga.createOrReplaceTempView('manga')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple count\n",
    "This is the first query, so there will be some lazy allocations and OS level page caches will most likely be unrelated and populated here. After this, the same query should execute faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3757380|\n",
      "+--------+\n",
      "\n",
      "16.033992290496826"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "spark.sql('SELECT count(*) FROM manga').show()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3757380|\n",
      "+--------+\n",
      "\n",
      "6.466812610626221"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "spark.sql('SELECT count(*) FROM manga').show()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counting fiber-exposure per ifu\n",
    "The previous query could benefit a lot from parquet column group statistics (you can look at the spark task, it reads only a small portion of the entire dataset - indeed only ~0.2% of the dataset is read). The query still benefits strongly from parquet format, due to run-length-encoding (RLE), but there is a larger burden on computation due to grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|PLATEID|IFUDESIGN|fexps|\n",
      "+-------+---------+-----+\n",
      "|   7964|    12705| 1905|\n",
      "|   9507|     9101|  819|\n",
      "|   7443|    12701| 1905|\n",
      "|   8721|    12705| 1143|\n",
      "|   8085|    12703| 1524|\n",
      "|   8262|    12701| 1143|\n",
      "|   9881|     3702|  333|\n",
      "|   8084|    12704| 1905|\n",
      "|   9195|     1902|  228|\n",
      "|   8315|     3701|  333|\n",
      "|   9507|    12703| 1143|\n",
      "|   8261|     3701|  333|\n",
      "|   8993|     3702|  333|\n",
      "|   8323|     3702|  333|\n",
      "|   8486|     3702|  333|\n",
      "|   8440|     6104|  732|\n",
      "|   9675|     6102|  366|\n",
      "|   9675|     6103|  366|\n",
      "|   8728|    12701| 1524|\n",
      "|   9049|     3703|  444|\n",
      "+-------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "14.67196774482727"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "spark.sql('SELECT PLATEID,IFUDESIGN,count(*) AS fexps FROM manga GROUP BY PLATEID,IFUDESIGN').show()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band average flux\n",
    "Here we force the system to scan columns that make little/no benefit from RLE, forcing a significant read. However, as we are only interacting with 5 columns, 3 of which are largely RLE'd away, we end up reading about 23% of the whole dataset, and utilize cpu for grouping/aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+------------------------+\n",
      "|PLATEID|IFUDESIGN| MANGAID|avg(SPEC.FLUX AS `FLUX`)|\n",
      "+-------+---------+--------+------------------------+\n",
      "|   8320|     3701|1-519738|      2.7885750267017317|\n",
      "|   9185|     6103|1-546965|     0.31238629822468966|\n",
      "|   8933|     6103|1-456768|      1.2990703250866296|\n",
      "|   8452|    12702|1-147685|      0.1602110807781861|\n",
      "|   8552|     6102|1-321961|      0.4957944524832962|\n",
      "|   9051|    12703|    50-4|    0.043823784664009144|\n",
      "|   8450|     1902|1-491220|      1.6152788146299577|\n",
      "|   8942|    12703|1-218519|        0.22637839739722|\n",
      "|   8716|     3704|1-352081|     0.24095725655764183|\n",
      "|   8945|    12702|1-615167|      1.3982744608175408|\n",
      "|   8595|    12701|1-197805|      0.1740635495507865|\n",
      "|   8548|     6102| 1-93236|      0.8367524575518326|\n",
      "|   8333|     9101|1-265919|      0.5213299778290923|\n",
      "|   8726|     9101|1-604826|      0.9012554880994621|\n",
      "|   8458|    12704|1-166908|     0.36139197538068496|\n",
      "|   8952|    12702|1-627000|      0.4005146534306271|\n",
      "|   8606|     9101|1-634274|      0.6764116891956877|\n",
      "|   8486|     3704|1-209607|       0.373796939024433|\n",
      "|   9863|     3704|1-456418|      1.3975573590253365|\n",
      "|   9095|     9101|1-317837|      0.5308839687195291|\n",
      "+-------+---------+--------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "311.39486622810364"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "spark.sql(\n",
    "'''\n",
    "SELECT PLATEID, IFUDESIGN, MANGAID, avg(SPEC.FLUX)\n",
    "FROM (\n",
    "    SELECT explode_outer(arrays_zip(WAVE, FLUX)) AS SPEC,\n",
    "    PLATEID, IFUDESIGN, MANGAID\n",
    "    FROM manga\n",
    ")\n",
    "WHERE SPEC.WAVE BETWEEN 3700 AND 4000\n",
    "GROUP BY PLATEID, IFUDESIGN, MANGAID\n",
    "'''\n",
    ").show()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup after ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
