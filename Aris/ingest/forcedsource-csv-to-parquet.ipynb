{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# Spark sample showing read/write methods\nIn this sample notebook, we will read CSV file from HDFS, write it as parquet file and save a Hive table definition. We will also run some Spark SQL commands using the Hive table.\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "%%configure -f\r\n{\"executorMemory\": \"12g\", \"executorCores\": 4, \"numExecutors\":11}\r\n",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "for item in sorted(sc._conf.getAll()): print(item)\r\n# vanilla:\r\n#('spark.livy.spark_major_version', '2')\r\n#('spark.master', 'yarn')",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# jvv: Meet's schema, tweaked for with UC's sql<->parquet data type mappings for stuff like INTs and BITs\r\n# SQL BIT as parquet booleans here\r\nfrom pyspark.sql.types import *\r\n\r\n# Source:\r\ncustomSchemaint= StructType([ \r\n      StructField(\"deepSourceId\", LongType(), True),\r\n      StructField(\"scienceCcdExposureId\", LongType(), True),\r\n      StructField(\"psfFlux\", DoubleType(), True),\r\n      StructField(\"psfFluxSigma\", DoubleType(), True),\r\n      StructField(\"flagBadMeasCentroid\", IntegerType(), True),\r\n      StructField(\"flagPixEdge\", IntegerType(), True),\r\n      StructField(\"flagPixInterpAny\", IntegerType(), True),\r\n      StructField(\"flagPixInterpCen\", IntegerType(), True),\r\n      StructField(\"flagPixSaturAny\", IntegerType(), True),\r\n      StructField(\"flagPixSaturCen\", IntegerType(), True),\r\n      StructField(\"flagBadPsfFlux\", IntegerType(), True),\r\n      StructField(\"chunkId\", IntegerType(), True),\r\n      StructField(\"subChunkId\", IntegerType(), True)\r\n\r\n\r\n]) \r\n",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1560370594170_0001</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://172.23.25.64:30443/gateway/default/yarn/proxy/application_1560370594170_0001/\">Link</a></td><td><a target=\"_blank\" href=\"https://172.23.25.64:30443/gateway/default/yarn/container/container_1560370594170_0001_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "#df1 = spark.read.load('/LSST/Source/csv/', format=\"csv\", sep=';', schema=customSchema, header=\"true\")\r\n#df1 = spark.read.load('/LSST/Source/csv/Source_8945.csv', format=\"csv\", sep=';' , schema=customSchema)\r\n#df1 = spark.read.load('/LSST/Source/csv/Source_8945.csv', format=\"csv\", sep=';', inferSchema=\"true\", header=\"true\")\r\n#df1 = spark.read.load('/LSST/Source/csv/Source_8945.csv', format=\"csv\", sep=';', schema=customSchema, header=\"true\")\r\ndf1 = spark.read.load('/LSST/sue/csv/forcedsource', format=\"csv\", sep=';', schema=customSchemaint, header=\"true\")\r\n#df1 = spark.read.load('/LSST/sue/csv/Source/Source_9659.csv', format=\"csv\", sep=',', inferSchema=\"true\")\r\n#df1 = spark.read.load('/LSST/sue/csv/Source/Source_9659.csv', format=\"csv\", sep=',', schema=customSchemaint)\r\n#df1 = spark.read.load('/LSST/jvv/Source_9659_id-chunkid-coord_ra.csv', format=\"csv\", sep=',', schema=customSchema)\r\n#df1 = spark.read.load('/LSST/jvv/csv/Source', format=\"csv\", sep=',', schema=customSchema)\r\n#df1 = spark.read.load('/LSST/jvv/test', format=\"csv\", sep=',', schema=customSchema)\r\n\r\n# Aris sample, not identical to Meet's:\r\n'''\r\nresults = spark.read.option(\"inferSchema\", \"true\").csv('/clickstream_data').toDF(\r\n            \"wcs_click_date_sk\", \"wcs_click_time_sk\", \"wcs_sales_sk\", \"wcs_item_sk\", \"wcs_web_page_sk\", \"wcs_user_sk\"\r\n            )\r\n'''\r\n\r\n#df1.printSchema()\r\ndf1.show()",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "+----------------+--------------------+------------+------------+-------------------+-----------+----------------+----------------+---------------+---------------+--------------+-------+----------+\n|    deepSourceId|scienceCcdExposureId|     psfFlux|psfFluxSigma|flagBadMeasCentroid|flagPixEdge|flagPixInterpAny|flagPixInterpCen|flagPixSaturAny|flagPixSaturCen|flagBadPsfFlux|chunkId|subChunkId|\n+----------------+--------------------+------------+------------+-------------------+-----------+----------------+----------------+---------------+---------------+--------------+-------+----------+\n|3166692272243625|          6471250146| 19.31360054|38.270599365|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243472|          6471250146|258.83999634|38.578201294|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245308|          6471250146|194.63900757|38.440898895|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243489|          6471250146|194.62199402|38.440700531|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243477|          6471250146|-14.85280037|37.514801025|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243493|          6471250146|73.769500732|38.052398682|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245532|          6471250146|65.035797119|37.786998749|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245472|          6471250146|161.68600464|38.173900604|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245464|          6471250146|195.52600098|38.442401886|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245452|          6471250146|101.11000061|38.101398468|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245922|          6471250146|-37.69219971|37.914600372|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243635|          6471250146|-44.82559967|37.968700409|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243634|          6471250146|-6.639150143|38.168399811|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243622|          6471250146|-46.29840088|37.875099182|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243500|          6471250146|41.558601379| 37.79309845|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243630|          6471250146|-0.084852003|38.280799866|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243627|          6471250146|114.61000061|38.710800171|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243629|          6471250146|-46.09500122|38.028701782|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272243632|          6471250146|14.972599983|38.307498932|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n|3166692272245874|          6471250146|39.507598877|38.222198486|                  0|          0|               0|               0|              0|              0|             0|      0|       746|\n+----------------+--------------------+------------+------------+-------------------+-----------+----------------+----------------+---------------+---------------+--------------+-------+----------+\nonly showing top 20 rows",
                    "output_type": "stream"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": "#df1.coalesce(350).write.parquet(\"/user/hive/warehouse/forcedsource/\", mode='overwrite')\r\n\r\n# Meet above v. Aris sample:\r\n# results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"web_clickstreams\")\r\ndf1.coalesce(350).write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"forcedsource\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": "df1.printSchema()",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": "# Disable saving SUCCESS file\r\nsc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \r\n\r\n# Print the current warehouse directory where the parquet files will be stored\r\nprint(spark.conf.get(\"spark.sql.warehouse.dir\"))\r\n\r\n# Save results as parquet file and create hive table\r\nresults.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"web_clickstreams\")\r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "import datetime\r\n\r\nbefore = datetime.datetime.now()\r\n\r\n\r\n# Execute Spark SQL commands\r\n#sqlDF = spark.sql(\"SELECT * FROM Source LIMIT 100\")\r\n\r\nsqlDF = spark.sql(\"SELECT count(*)  FROM Source where flux_sinc between 1 and 1.1\")\r\n\r\nsqlDF.show()\r\nafter = datetime.datetime.now()\r\nprint (after - before )\r\n\r\n#sqlDF = spark.sql(\"SELECT wcs_user_sk, COUNT(*)\\\r\n#                     FROM web_clickstreams\\\r\n#                    WHERE wcs_user_sk IS NOT NULL\\\r\n#                   GROUP BY wcs_user_sk\\\r\n#                   ORDER BY COUNT(*) DESC LIMIT 100\")\r\n#sqlDF.show()",
            "metadata": {},
            "outputs": [],
            "execution_count": 19
        },
        {
            "cell_type": "code",
            "source": "import datetime\r\n\r\nbefore = datetime.datetime.now()\r\ntime.sleep(1)\r\nafter = datetime.datetime.now()\r\nprint (after - before )",
            "metadata": {},
            "outputs": [],
            "execution_count": 16
        },
        {
            "cell_type": "code",
            "source": "import pyspark\r\n\r\n# start\r\nsc = pyspark.SparkContext()\r\n\r\n#stop\r\nsc.stop()",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# Read the product reviews CSV files into a spark data frame, print schema & top rows\r\nresults = spark.read.option(\"inferSchema\", \"true\").csv('/product_review_data').toDF(\r\n            \"pr_review_sk\", \"pr_review_content\"\r\n            )\r\nresults.printSchema()\r\nresults.show()",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# Save results as parquet file and create hive table\r\nresults.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"product_reviews\")\r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# Execute Spark SQL commands\r\nsqlDF = spark.sql(\"SELECT pr_review_sk, CHAR_LENGTH(pr_review_content) as len FROM product_reviews LIMIT 100\")\r\nsqlDF.show()",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 1
        }
    ]
}