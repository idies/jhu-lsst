{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala"
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%%configure -f\r\n",
                "{\"executorMemory\": \"12g\", \"executorCores\": 4, \"numExecutors\":11}"
            ],
            "metadata": {
                "azdata_cell_guid": "7264da71-4415-4742-bb09-f303d947178c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "Current session configs: <tt>{'executorMemory': '12g', 'executorCores': 4, 'numExecutors': 11, 'kind': 'spark'}</tt><br>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1580142637008_0008</td><td>pyspark</td><td>killed</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/cluster/app/application_1580142637008_0008\">Link</a></td><td></td><td></td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "%%configure -f\n",
                "{\"conf\": {\"spark.jars\": \"/system/jar/simpleHTM.jar,/system/jar/jhealpix.jar\"}}"
            ],
            "metadata": {
                "azdata_cell_guid": "08ecabb4-1d35-4fb6-939d-74de2fa55210"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "Current session configs: <tt>{'conf': {'spark.jars': '/system/jar/simpleHTM.jar,/system/jar/jhealpix.jar'}, 'kind': 'spark'}</tt><br>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1580142637008_0008</td><td>pyspark</td><td>killed</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/cluster/app/application_1580142637008_0008\">Link</a></td><td></td><td></td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "import simple.HTMindex\n",
                "import healpix.jhu.Healpix\n",
                "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\n",
                "import org.apache.spark.sql.functions.udf\n",
                "import java.util.Calendar\n",
                "\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "700b57ad-e424-46d1-88ca-0257d1097e4e"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1580142637008_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://172.23.25.61:30443/gateway/default/yarn/proxy/application_1580142637008_0010/\">Link</a></td><td><a target=\"_blank\" href=\"https://172.23.25.61:30443/gateway/default/yarn/container/container_1580142637008_0010_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "391158194d664112ab41995d4808ef17"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "4d2abec57a504537a88f977f57efe61c"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "import simple.HTMindex\nimport healpix.jhu.Healpix\nimport org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\nimport org.apache.spark.sql.functions.udf\nimport java.util.Calendar\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "object HTMUtils extends Serializable {\r\n",
                "    var htmindex= new HTMindex() with Serializable\r\n",
                "    val htmid: (Double, Double) => Long = htmindex.lookupId(_,_)\r\n",
                "\r\n",
                "    val htmidUDF=udf(htmid)\r\n",
                "    \r\n",
                "}\n",
                "\n",
                "object HEALPixUtils extends Serializable {\n",
                "    var hp = new Healpix() with Serializable\n",
                "    val healpixid: (Double, Double) => Long = hp.ang2pix(_,_)\n",
                "    val healpixidUDF=udf(healpixid)\n",
                "    \n",
                "}"
            ],
            "metadata": {
                "azdata_cell_guid": "9b93909d-741b-4c62-a375-dff74aae8e00"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "cade850b21b64cb68df98d715c15e747"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "defined object HTMUtils\ndefined object HEALPixUtils\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "read object table parquet files into dataframe (objDF)\n",
                "\n",
                "create new dataframe (newDF) with calculated htmid"
            ],
            "metadata": {
                "azdata_cell_guid": "e4015c93-de42-4dc0-8e95-26ac6fd067ac"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "val datafile = \"/user/hive/warehouse/object_test\"\r\n",
                "val objDF = spark.read.parquet(datafile)\r\n",
                "\r\n",
                "val newDF = objDF.withColumn(\"htmid\",HTMUtils.htmidUDF(objDF(\"ra\"),objDF(\"decl\"))).withColumn(\"healpixid\", HEALPixUtils.healpixidUDF(objDF(\"ra\"), objDF(\"decl\")))\r\n",
                "\r\n",
                "//val newDF = objDF.withColumn(\"htmid\",HTMUtils.htmidUDF(objDF(\"ra\"),objDF(\"decl\")))\r\n",
                "//newDF.printSchema()\r\n",
                "//newDF.show(1)"
            ],
            "metadata": {
                "azdata_cell_guid": "1446a582-e50a-41c1-a434-d4f2ab2750bc"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "767e8b68c79342ecbb5e90928839ee5c"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "datafile: String = /user/hive/warehouse/object_test\nobjDF: org.apache.spark.sql.DataFrame = [deepSourceId: bigint, ra: double ... 234 more fields]\nnewDF: org.apache.spark.sql.DataFrame = [deepSourceId: bigint, ra: double ... 236 more fields]\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "write newDF to external table in SqlDataPool using Spark-SQL connector\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "2ee61926-8b14-4226-8660-5e91c546f0ff"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "\r\n",
                "println(\"Use MSSQL connector to write to master SQL instance \")\r\n",
                "\r\n",
                "val servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
                "val dbname = \"LSST\"\r\n",
                "var url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "\r\n",
                "\r\n",
                "val user = \"admin\"\r\n",
                "val password = \"fooRiuzg54\"\r\n",
                "\r\n",
                "//val password = \"fakefake\"\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "//val user = \"spark_user\"\r\n",
                "//val password = \"!!foospark9999\" //Please specify password here\r\n",
                "\r\n",
                "val datapool_table = \"Object_WTF2\"\r\n",
                "\r\n",
                "val datasource_name = \"SqlDataPool\"\r\n",
                "\r\n",
                "val batchsize = 1000000\r\n",
                "\r\n",
                "val start = Calendar.getInstance().getTime()\r\n",
                "\r\n",
                "try {\r\n",
                "  newDF.write \r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \r\n",
                "    .mode(\"append\") \r\n",
                "    .option(\"url\", url) \r\n",
                "    .option(\"dbtable\", datapool_table) \r\n",
                "    .option(\"user\", user) \r\n",
                "    .option(\"password\", password) \r\n",
                "    .option(\"dataPoolDataSource\",datasource_name)\r\n",
                "    .option(\"batchsize\",batchsize)\r\n",
                "    .save()\r\n",
                "} catch {\r\n",
                "    case e: Throwable => println(\"MSSQL Connector write failed: \" + e)\r\n",
                "}\r\n",
                "val end = Calendar.getInstance().getTime()\r\n",
                "println(start)\r\n",
                "println(end)\r\n",
                "//var duration = end - start\r\n",
                "//println(\"duration:\" + duration)\r\n",
                "print(\"MSSQL Connector write(append) to data pool external table succeeded\")"
            ],
            "metadata": {
                "azdata_cell_guid": "16d5da7f-9510-4121-b0ea-7ace227b005c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "abc08e792a3e4a54a79067ccd6354f74"
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "Use MSSQL connector to write to master SQL instance \nservername: String = jdbc:sqlserver://master-0.master-svc\ndbname: String = LSST\nurl: String = jdbc:sqlserver://master-0.master-svc;databaseName=LSST;\nuser: String = admin\npassword: String = fooRiuzg54\ndatapool_table: String = Object_WTF2\ndatasource_name: String = SqlDataPool\nbatchsize: Int = 1000000\nstart: java.util.Date = Mon Jan 27 20:10:53 UTC 2020\nMSSQL Connector write failed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 90 in stage 1.0 failed 4 times, most recent failure: Lost task 90.3 in stage 1.0 (TID 177, storage-0-1.storage-0-svc.filedb.svc.cluster.local, executor 1): com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'admin'. ClientConnectionId:3b9db57d-c46b-42ba-847d-1de5c99bf4c4\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:254)\n\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:258)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:104)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:4772)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3581)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:81)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3541)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7240)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2869)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2395)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:80)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:58)\n\tat com.microsoft.sqlserver.jdbc.spark.BulkCopyUtils$.savePartition(BulkCopyUtils.scala:37)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:123)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:112)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\nend: java.util.Date = Mon Jan 27 20:10:59 UTC 2020\nMon Jan 27 20:10:53 UTC 2020\nMon Jan 27 20:10:59 UTC 2020\nMSSQL Connector write(append) to data pool external table succeeded",
                    "output_type": "stream"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "5439cd10-0c2c-4dd7-bd15-ef3a56511a14"
            },
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "3c6e19db-f160-4a6e-91d6-07c615823895"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}