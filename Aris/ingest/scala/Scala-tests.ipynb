{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala"
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "val textFile = spark.read.textFile(\"/LSST/csv/test/Object/Object_9991.csv\")\r\n",
                "\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "cecd76fd-4de4-4dcf-a22c-77b372039d91"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "name": "stderr",
                    "text": "The code failed because of a fatal error:\n\tSession 2 unexpectedly reached final status 'dead'. See logs:\nstdout: \n\nstderr: \n2020-01-24 05:50:08,386 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-api-0.6.78235.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-rsc-0.6.78235.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/netty-all-4.1.17.Final.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-api-jdo-3.2.6.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-core-3.2.10.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-rdbms-3.2.9.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/commons-codec-1.9.jar.\n2020-01-24 05:50:09,661 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-core_2.11-0.6.78235.jar.\n2020-01-24 05:50:09,661 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-repl_2.11-0.6.78235.jar.\n2020-01-24 05:50:10,038 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers\n2020-01-24 05:50:10,147 INFO conf.Configuration: resource-types.xml not found\n2020-01-24 05:50:10,148 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n2020-01-24 05:50:10,168 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)\n2020-01-24 05:50:10,169 INFO yarn.Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n2020-01-24 05:50:10,170 INFO yarn.Client: Setting up container launch context for our AM\n2020-01-24 05:50:10,176 INFO yarn.Client: Setting up the launch environment for our AM container\n2020-01-24 05:50:10,188 INFO yarn.Client: Preparing resources for our AM container\n2020-01-24 05:50:10,211 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/system/spark/spark_libs.zip\n2020-01-24 05:50:10,273 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-api-0.6.78235.jar\n2020-01-24 05:50:10,281 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-rsc-0.6.78235.jar\n2020-01-24 05:50:10,283 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/netty-all-4.1.17.Final.jar\n2020-01-24 05:50:10,287 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-api-jdo-3.2.6.jar\n2020-01-24 05:50:10,294 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-core-3.2.10.jar\n2020-01-24 05:50:10,297 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-rdbms-3.2.9.jar\n2020-01-24 05:50:10,300 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/commons-codec-1.9.jar\n2020-01-24 05:50:10,308 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-core_2.11-0.6.78235.jar\n2020-01-24 05:50:10,310 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-repl_2.11-0.6.78235.jar\n2020-01-24 05:50:10,313 WARN yarn.Client: Same name resource file:///opt/spark/jars/datanucleus-api-jdo-3.2.6.jar added multiple times to distributed cache\n2020-01-24 05:50:10,314 WARN yarn.Client: Same name resource file:///opt/spark/jars/datanucleus-core-3.2.10.jar added multiple times to distributed cache\n2020-01-24 05:50:10,314 WARN yarn.Client: Same name resource file:///opt/spark/jars/datanucleus-rdbms-3.2.9.jar added multiple times to distributed cache\n2020-01-24 05:50:10,315 INFO yarn.Client: Uploading resource file:/opt/spark/conf/hive-site.xml -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/hive-site.xml\n2020-01-24 05:50:10,634 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/system/livy/sparkr.zip#sparkr\n2020-01-24 05:50:10,641 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/pyspark.zip -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/pyspark.zip\n2020-01-24 05:50:10,717 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/py4j-0.10.7-src.zip\n2020-01-24 05:50:10,748 WARN yarn.Client: Same name resource hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/pyspark.zip added multiple times to distributed cache\n2020-01-24 05:50:10,748 WARN yarn.Client: Same name resource hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/py4j-0.10.7-src.zip added multiple times to distributed cache\n2020-01-24 05:50:10,940 INFO yarn.Client: Uploading resource file:/tmp/spark-51df2035-c7b0-47eb-8670-3c4603a2bfe8/__spark_conf__5791247512745070095.zip -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/__spark_conf__.zip\n2020-01-24 05:50:11,139 INFO spark.SecurityManager: Changing view acls to: root\n2020-01-24 05:50:11,140 INFO spark.SecurityManager: Changing modify acls to: root\n2020-01-24 05:50:11,141 INFO spark.SecurityManager: Changing view acls groups to: \n2020-01-24 05:50:11,141 INFO spark.SecurityManager: Changing modify acls groups to: \n2020-01-24 05:50:11,142 INFO spark.SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n2020-01-24 05:50:12,377 INFO yarn.Client: Submitting application application_1579817842874_0003 to ResourceManager\n2020-01-24 05:50:12,436 INFO impl.YarnClientImpl: Submitted application application_1579817842874_0003\n2020-01-24 05:50:12,439 INFO yarn.Client: Application report for application_1579817842874_0003 (state: ACCEPTED)\n2020-01-24 05:50:12,443 INFO yarn.Client: \n\t client token: N/A\n\t diagnostics: [Fri Jan 24 05:50:12 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1579845012399\n\t final status: UNDEFINED\n\t tracking URL: https://sparkhead-0.sparkhead-svc:8090/proxy/application_1579817842874_0003/\n\t user: root\n2020-01-24 05:50:12,447 INFO util.ShutdownHookManager: Shutdown hook called\n2020-01-24 05:50:12,448 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-51df2035-c7b0-47eb-8670-3c4603a2bfe8\n2020-01-24 05:50:12,453 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-efd2d253-3171-4872-8d52-0655294c12aa\n\nYARN Diagnostics: \nMax number of executor failures (22) reached.\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "textFile.first()"
            ],
            "metadata": {
                "azdata_cell_guid": "d3e7143b-7959-4b6b-9eea-dd5fdf50b4be"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "text": "The code failed because of a fatal error:\n\tSession 2 unexpectedly reached final status 'dead'. See logs:\nstdout: \n\nstderr: \n2020-01-24 05:50:08,386 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-api-0.6.78235.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-rsc-0.6.78235.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/netty-all-4.1.17.Final.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-api-jdo-3.2.6.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-core-3.2.10.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-rdbms-3.2.9.jar.\n2020-01-24 05:50:09,660 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/commons-codec-1.9.jar.\n2020-01-24 05:50:09,661 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-core_2.11-0.6.78235.jar.\n2020-01-24 05:50:09,661 WARN deploy.DependencyUtils: Skip remote jar hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-repl_2.11-0.6.78235.jar.\n2020-01-24 05:50:10,038 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers\n2020-01-24 05:50:10,147 INFO conf.Configuration: resource-types.xml not found\n2020-01-24 05:50:10,148 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n2020-01-24 05:50:10,168 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)\n2020-01-24 05:50:10,169 INFO yarn.Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n2020-01-24 05:50:10,170 INFO yarn.Client: Setting up container launch context for our AM\n2020-01-24 05:50:10,176 INFO yarn.Client: Setting up the launch environment for our AM container\n2020-01-24 05:50:10,188 INFO yarn.Client: Preparing resources for our AM container\n2020-01-24 05:50:10,211 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/system/spark/spark_libs.zip\n2020-01-24 05:50:10,273 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-api-0.6.78235.jar\n2020-01-24 05:50:10,281 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/livy-rsc-0.6.78235.jar\n2020-01-24 05:50:10,283 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/rsc-jars/netty-all-4.1.17.Final.jar\n2020-01-24 05:50:10,287 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-api-jdo-3.2.6.jar\n2020-01-24 05:50:10,294 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-core-3.2.10.jar\n2020-01-24 05:50:10,297 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/spark/datanucleus-rdbms-3.2.9.jar\n2020-01-24 05:50:10,300 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/commons-codec-1.9.jar\n2020-01-24 05:50:10,308 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-core_2.11-0.6.78235.jar\n2020-01-24 05:50:10,310 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/repl_2.11-jars/livy-repl_2.11-0.6.78235.jar\n2020-01-24 05:50:10,313 WARN yarn.Client: Same name resource file:///opt/spark/jars/datanucleus-api-jdo-3.2.6.jar added multiple times to distributed cache\n2020-01-24 05:50:10,314 WARN yarn.Client: Same name resource file:///opt/spark/jars/datanucleus-core-3.2.10.jar added multiple times to distributed cache\n2020-01-24 05:50:10,314 WARN yarn.Client: Same name resource file:///opt/spark/jars/datanucleus-rdbms-3.2.9.jar added multiple times to distributed cache\n2020-01-24 05:50:10,315 INFO yarn.Client: Uploading resource file:/opt/spark/conf/hive-site.xml -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/hive-site.xml\n2020-01-24 05:50:10,634 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/system/livy/sparkr.zip#sparkr\n2020-01-24 05:50:10,641 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/pyspark.zip -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/pyspark.zip\n2020-01-24 05:50:10,717 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/py4j-0.10.7-src.zip\n2020-01-24 05:50:10,748 WARN yarn.Client: Same name resource hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/pyspark.zip added multiple times to distributed cache\n2020-01-24 05:50:10,748 WARN yarn.Client: Same name resource hdfs://nmnode-0-0.nmnode-0-svc:9000/system/livy/py4j-0.10.7-src.zip added multiple times to distributed cache\n2020-01-24 05:50:10,940 INFO yarn.Client: Uploading resource file:/tmp/spark-51df2035-c7b0-47eb-8670-3c4603a2bfe8/__spark_conf__5791247512745070095.zip -> hdfs://nmnode-0-0.nmnode-0-svc:9000/user/root/.sparkStaging/application_1579817842874_0003/__spark_conf__.zip\n2020-01-24 05:50:11,139 INFO spark.SecurityManager: Changing view acls to: root\n2020-01-24 05:50:11,140 INFO spark.SecurityManager: Changing modify acls to: root\n2020-01-24 05:50:11,141 INFO spark.SecurityManager: Changing view acls groups to: \n2020-01-24 05:50:11,141 INFO spark.SecurityManager: Changing modify acls groups to: \n2020-01-24 05:50:11,142 INFO spark.SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n2020-01-24 05:50:12,377 INFO yarn.Client: Submitting application application_1579817842874_0003 to ResourceManager\n2020-01-24 05:50:12,436 INFO impl.YarnClientImpl: Submitted application application_1579817842874_0003\n2020-01-24 05:50:12,439 INFO yarn.Client: Application report for application_1579817842874_0003 (state: ACCEPTED)\n2020-01-24 05:50:12,443 INFO yarn.Client: \n\t client token: N/A\n\t diagnostics: [Fri Jan 24 05:50:12 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1579845012399\n\t final status: UNDEFINED\n\t tracking URL: https://sparkhead-0.sparkhead-svc:8090/proxy/application_1579817842874_0003/\n\t user: root\n2020-01-24 05:50:12,447 INFO util.ShutdownHookManager: Shutdown hook called\n2020-01-24 05:50:12,448 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-51df2035-c7b0-47eb-8670-3c4603a2bfe8\n2020-01-24 05:50:12,453 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-efd2d253-3171-4872-8d52-0655294c12aa\n\nYARN Diagnostics: \nMax number of executor failures (22) reached.\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "textFile.head(3)"
            ],
            "metadata": {
                "azdata_cell_guid": "b62a965a-c2ad-43d3-8a9a-9fdb7d494854"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "spark\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "08ecabb4-1d35-4fb6-939d-74de2fa55210"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "var df = spark.read.format(",
                ").option(",
                ", ",
                false,
                ").load(\"/LSST/csv/Object/Object_0.csv\")"
            ],
            "metadata": {
                "azdata_cell_guid": "06b8578f-4f9e-4dfe-9854-73c3cf86f426"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "spark.sparkContext.getConf().driver"
            ],
            "metadata": {
                "azdata_cell_guid": "882bfbce-343a-4aad-aede-cc73dcd05ed4"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}