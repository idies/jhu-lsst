{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "7264da71-4415-4742-bb09-f303d947178c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorMemory': '12g', 'executorCores': 4, 'numExecutors': 11, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\r\n",
    "{\"executorMemory\": \"12g\", \"executorCores\": 4, \"numExecutors\":11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "azdata_cell_guid": "08ecabb4-1d35-4fb6-939d-74de2fa55210"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': '/system/jar/simpleHTM.jar,/system/jar/jhealpix.jar'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\"spark.jars\": \"/system/jar/simpleHTM.jar,/system/jar/jhealpix.jar\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "azdata_cell_guid": "700b57ad-e424-46d1-88ca-0257d1097e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>21</td><td>application_1580142637008_0022</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://172.23.25.61:30443/gateway/default/yarn/proxy/application_1580142637008_0022/\">Link</a></td><td><a target=\"_blank\" href=\"https://172.23.25.61:30443/gateway/default/yarn/container/container_1580142637008_0022_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936ebf62d6604606a52b89ae002b7ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db80b9a2b52e4d3881ce2da416c3b6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import simple.HTMindex\n",
      "import healpix.jhu.Healpix\n",
      "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\n",
      "import org.apache.spark.sql.functions.udf\n",
      "import java.util.Calendar\n"
     ]
    }
   ],
   "source": [
    "import simple.HTMindex\n",
    "import healpix.jhu.Healpix\n",
    "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import java.util.Calendar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# can we get this login thing to work in scala?\n",
    "this was a pain, it would be better if i ACTUALL knew scala i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6698b2c2b3b46e2987339da70467d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.rdd.RDD\n",
      "authDF: org.apache.spark.sql.DataFrame = [jdbc_password: string, jdbc_username: string]\n",
      "m: Map[String,Any] = Map(jdbc_password -> fooRiuzg54, jdbc_username -> admin)\n",
      "username: Any = admin\n",
      "password: Any = fooRiuzg54\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val authDF = spark.read.json(\"hdfs:///.config/creds.json\")\n",
    "val m = authDF.first.getValuesMap[Any](authDF.schema.fieldNames)\n",
    "\n",
    "val username = (m(\"jdbc_username\"))\n",
    "//println(username)\n",
    "val password = (m(\"jdbc_password\"))\n",
    "//println(password)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e4015c93-de42-4dc0-8e95-26ac6fd067ac"
   },
   "source": [
    "read object table parquet files into dataframe (objDF)\n",
    "\n",
    "create new dataframe (newDF) with calculated htmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "azdata_cell_guid": "1446a582-e50a-41c1-a434-d4f2ab2750bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708908cfcacd4c39945190cfd7ce6448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datafile: String = /user/hive/warehouse/object\n",
      "objDF: org.apache.spark.sql.DataFrame = [deepSourceId: bigint, ra: double ... 234 more fields]\n",
      "newDF: org.apache.spark.sql.DataFrame = [deepSourceId: bigint, ra: double ... 236 more fields]\n"
     ]
    }
   ],
   "source": [
    "val datafile = \"/user/hive/warehouse/object_n\"\r\n",
    "val bjdfF = spark.read.parquet(datafile)\r",
    "//\n",
    "\r\n",
    "val newDF = objDF.withColumn(\"htmid\",HTMUtils.htmidUDF(objDF(\"ra\"),objDF(\"decl\"))).withColumn(\"healpixid\", HEALPixUtils.healpixidUDF(objDF(\"ra\"), objDF(\"decl\")))\r\n",
    "\r\n",
    "//val newDF = objDF.withColumn(\"htmid\",HTMUtils.htmidUDF(objDF(\"ra\"),objDF(\"decl\")))\r\n",
    "//newDF.printSchema()\r\n",
    "//newDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2ee61926-8b14-4226-8660-5e91c546f0ff"
   },
   "source": [
    "write newDF to external table in SqlDataPool using Spark-SQL connector\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "16d5da7f-9510-4121-b0ea-7ace227b005c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e22fff33b7481cacfaea9db8b903e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use MSSQL connector to write to master SQL instance \n",
      "servername: String = jdbc:sqlserver://master-0.master-svc\n",
      "dbname: String = LSST\n",
      "url: String = jdbc:sqlserver://master-0.master-svc;databaseName=LSST;\n",
      "user: String = admin\n",
      "password: String = fooRiuzg54\n",
      "datapool_table: String = dp.Object\n",
      "datasource_name: String = SqlDataPool\n",
      "batchsize: Int = 1000000\n",
      "start: java.util.Date = Tue Jan 28 04:51:13 UTC 2020\n",
      "end: java.util.Date = Tue Jan 28 08:00:19 UTC 2020\n",
      "Tue Jan 28 04:51:13 UTC 2020\n",
      "Tue Jan 28 08:00:19 UTC 2020\n",
      "MSSQL Connector write(append) to data pool external table succeeded"
     ]
    }
   ],
   "source": [
    "\r\n",
    "println(\"Use MSSQL connector to write to master SQL instance \")\r\n",
    "\r\n",
    "val servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
    "val dbname = \"LSST\"\r\n",
    "var url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
    "\r\n",
    "\r\n",
    "val user = \"admin\"\r\n",
    "val password = \"fooRiuzg54\"\r\n",
    "\r\n",
    "//val password = \"fakefake\"\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "//val user = \"spark_user\"\r\n",
    "//val password = \"!!foospark9999\" //Please specify password here\r\n",
    "\r\n",
    "val datapool_table = \"dp.Object\"\r\n",
    "\r\n",
    "val datasource_name = \"SqlDataPool\"\r\n",
    "\r\n",
    "val batchsize = 1000000\r\n",
    "\r\n",
    "val start = Calendar.getInstance().getTime()\r\n",
    "\r\n",
    "try {\r\n",
    "  newDF.write \r\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \r\n",
    "    .mode(\"append\") \r\n",
    "    .option(\"url\", url) \r\n",
    "    .option(\"dbtable\", datapool_table) \r\n",
    "    .option(\"user\", user) \r\n",
    "    .option(\"password\", password) \r\n",
    "    .option(\"dataPoolDataSource\",datasource_name)\r\n",
    "    .option(\"batchsize\",batchsize)\r\n",
    "    .save()\r\n",
    "} catch {\r\n",
    "    case e: Throwable => println(\"MSSQL Connector write failed: \" + e)\r\n",
    "}\r\n",
    "val end = Calendar.getInstance().getTime()\r\n",
    "println(start)\r\n",
    "println(end)\r\n",
    "//var duration = end - start\r\n",
    "//println(\"duration:\" + duration)\r\n",
    "print(\"MSSQL Connector write(append) to data pool external table succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "5439cd10-0c2c-4dd7-bd15-ef3a56511a14"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "3c6e19db-f160-4a6e-91d6-07c615823895"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
