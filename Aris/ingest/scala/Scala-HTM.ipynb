{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala"
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%%configure -f\n",
                "{\"conf\": {\"spark.jars\": \"/jar/simpleHTM.jar\"}}"
            ],
            "metadata": {
                "azdata_cell_guid": "08ecabb4-1d35-4fb6-939d-74de2fa55210"
            },
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "import simple.HTMindex\r\n",
                "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\r\n",
                "import org.apache.spark.sql.functions.udf\r\n",
                "\r\n",
                "//var htmindex= new HTMindex()\r\n",
                "//val htmid: (Double, Double) => Long = htmindex.lookupId(_,_)\r\n",
                "\r\n",
                "//val htmidUDF=udf(htmid)"
            ],
            "metadata": {
                "azdata_cell_guid": "700b57ad-e424-46d1-88ca-0257d1097e4e"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "object HTMUtils extends Serializable {\r\n",
                "    var htmindex= new HTMindex() with Serializable\r\n",
                "    val htmid: (Double, Double) => Long = htmindex.lookupId(_,_)\r\n",
                "\r\n",
                "    val htmidUDF=udf(htmid)\r\n",
                "    \r\n",
                "}"
            ],
            "metadata": {
                "azdata_cell_guid": "9b93909d-741b-4c62-a375-dff74aae8e00"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "defined object HTMUtils\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 13
        },
        {
            "cell_type": "code",
            "source": [
                "val dataset = Seq((123.0,45.0), (124.0,46.0)).toDF(\"ra\", \"dec\")"
            ],
            "metadata": {
                "azdata_cell_guid": "3f810e8c-31f7-497f-8500-77d4492f7202"
            },
            "outputs": [],
            "execution_count": 14
        },
        {
            "cell_type": "code",
            "source": [
                "dataset.withColumn(\"htmid\", htmidUDF(dataset(\"ra\"),dataset(\"dec\"))).show"
            ],
            "metadata": {
                "azdata_cell_guid": "32fd8ae4-caca-4fa2-a17e-43bee81874c0"
            },
            "outputs": [],
            "execution_count": 21
        },
        {
            "cell_type": "code",
            "source": [
                "val datafile = \"/user/hive/warehouse/source_test_parquet\"\r\n",
                "val old_df = spark.read.parquet(datafile)\r\n",
                "\r\n",
                "old_df.createOrReplaceTempView(\"source\")\r\n",
                "\r\n",
                "val coordsDF = spark.sql(\"select coord_ra, coord_decl from source limit 10\")\r\n",
                "coordsDF.show()\r\n",
                "coordsDF.printSchema()\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "950502d4-f7c0-48ee-99af-0296aaf20849"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "datafile: String = /user/hive/warehouse/source_test_parquet\nold_df: org.apache.spark.sql.DataFrame = [id: bigint, chunkid: int ... 86 more fields]\ncoordsDF: org.apache.spark.sql.DataFrame = [coord_ra: double, coord_decl: double]\n+------------------+------------------+\n|          coord_ra|        coord_decl|\n+------------------+------------------+\n| 339.8799009480773| 34.60159626500321|\n| 339.9007831078834| 34.73830208759114|\n| 339.8350724565858| 34.65489429025321|\n| 339.9006981638171|34.741711297466075|\n|339.85114223901024| 34.60391043664569|\n| 339.9176012834339|34.733835899266445|\n|339.85677629540703|34.596872654451175|\n|340.03999850033125|  34.7732916046863|\n|339.86516714065425|34.606816308674404|\n| 339.9019848293452|34.727662874436874|\n+------------------+------------------+\n\nroot\n |-- coord_ra: double (nullable = true)\n |-- coord_decl: double (nullable = true)\n\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": [
                "val new_df = coordsDF.withColumn(\"htmid\",HTMUtils.htmidUDF(old_df(\"coord_ra\"),old_df(\"coord_decl\")))\r\n",
                "new_df.show(1)"
            ],
            "metadata": {
                "azdata_cell_guid": "ea25060d-daf3-431c-85f7-5b966661b358"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "new_df: org.apache.spark.sql.DataFrame = [coord_ra: double, coord_decl: double ... 1 more field]\n+------------------+-----------------+--------------+\n|          coord_ra|       coord_decl|         htmid|\n+------------------+-----------------+--------------+\n|357.03448646379684|33.41077013684323|13319061131005|\n+------------------+-----------------+--------------+\nonly showing top 1 row\n\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 14
        }
    ]
}