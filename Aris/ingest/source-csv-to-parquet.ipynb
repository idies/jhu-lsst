{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# Spark sample showing read/write methods\nIn this sample notebook, we will read CSV file from HDFS, write it as parquet file and save a Hive table definition. We will also run some Spark SQL commands using the Hive table.\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "%%configure -f\r\n{\"executorMemory\": \"58g\", \"executorCores\": 30, \"numExecutors\":4}\r\n",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": "for item in sorted(sc._conf.getAll()): print(item)\r\n# vanilla:\r\n#('spark.livy.spark_major_version', '2')\r\n#('spark.master', 'yarn')",
            "metadata": {},
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": "# this one doesn't work, don't use it.\r\n# jvv: Meet's schema, tweaked for with UC's sql<->parquet data type mappings for stuff like INTs and BITs\r\n# SQL BIT as parquet booleans here\r\nfrom pyspark.sql.types import *\r\n\r\n# Source:\r\ncustomSchemabool= StructType([ \r\n      StructField(\"id\", LongType(), True),\r\n      StructField(\"chunkid\", IntegerType(), True),\r\n      StructField(\"coord_ra\", DoubleType(), True),\r\n      StructField(\"coord_decl\", DoubleType(), True),\r\n      StructField(\"coord_htmId20\", LongType(), True),\r\n      StructField(\"parent\", LongType(), True),\r\n      StructField(\"flags_badcentroid\", BooleanType(), True),\r\n      StructField(\"centroid_sdss_x\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_y\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_xVar\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_xyCov\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_yVar\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_flags\", BooleanType(), True),\r\n      StructField(\"flags_pixel_edge\", BooleanType(), True),\r\n      StructField(\"flags_pixel_interpolated_any\", BooleanType(), True),\r\n      StructField(\"flags_pixel_interpolated_center\", BooleanType(), True),\r\n      StructField(\"flags_pixel_saturated_any\", BooleanType(), True),\r\n      StructField(\"flags_pixel_saturated_center\", BooleanType(), True),\r\n      StructField(\"flags_pixel_cr_any\", BooleanType(), True),\r\n      StructField(\"flags_pixel_cr_center\", BooleanType(), True),\r\n      StructField(\"centroid_naive_x\", DoubleType(), True),\r\n      StructField(\"centroid_naive_y\", DoubleType(), True),\r\n      StructField(\"centroid_naive_xVar\", DoubleType(), True),\r\n      StructField(\"centroid_naive_xyCov\", DoubleType(), True),\r\n      StructField(\"centroid_naive_yVar\", DoubleType(), True),\r\n      StructField(\"centroid_naive_flags\", BooleanType(), True),\r\n      StructField(\"centroid_gaussian_x\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_y\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_xVar\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_xyCov\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_yVar\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_flags\", BooleanType(), True),\r\n      StructField(\"shape_sdss_Ixx\", DoubleType(), True),\r\n      StructField(\"shape_sdss_Iyy\", DoubleType(), True),\r\n      StructField(\"shape_sdss_Ixy\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxxVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxxIyyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxxIxyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IyyVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IyyIxyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxyVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_flags\", BooleanType(), True),\r\n      StructField(\"shape_sdss_centroid_x\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_y\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_xVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_xyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_yVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_flags\", BooleanType(), True),\r\n      StructField(\"shape_sdss_flags_unweightedbad\", BooleanType(), True),\r\n      StructField(\"shape_sdss_flags_unweighted\", BooleanType(), True),\r\n      StructField(\"shape_sdss_flags_shift\", BooleanType(), True),\r\n      StructField(\"shape_sdss_flags_maxiter\", BooleanType(), True),\r\n      StructField(\"flux_psf\", DoubleType(), True),\r\n      StructField(\"flux_psf_err\", DoubleType(), True),\r\n      StructField(\"flux_psf_flags\", BooleanType(), True),\r\n      StructField(\"flux_psf_psffactor\", DoubleType(), True),\r\n      StructField(\"flux_psf_flags_psffactor\", BooleanType(), True),\r\n      StructField(\"flux_psf_flags_badcorr\", BooleanType(), True),\r\n      StructField(\"flux_naive\", DoubleType(), True),\r\n      StructField(\"flux_naive_err\", DoubleType(), True),\r\n      StructField(\"flux_naive_flags\", BooleanType(), True),\r\n      StructField(\"flux_gaussian\", DoubleType(), True),\r\n      StructField(\"flux_gaussian_err\", DoubleType(), True),\r\n      StructField(\"flux_gaussian_flags\", BooleanType(), True),\r\n      StructField(\"flux_gaussian_psffactor\", DoubleType(), True),\r\n      StructField(\"flux_gaussian_flags_psffactor\", BooleanType(), True),\r\n      StructField(\"flux_gaussian_flags_badcorr\", BooleanType(), True),\r\n      StructField(\"flux_sinc\", DoubleType(), True),\r\n      StructField(\"flux_sinc_err\", DoubleType(), True),\r\n      StructField(\"flux_sinc_flags\", BooleanType(), True),\r\n      StructField(\"centroid_record_x\", DoubleType(), True),\r\n      StructField(\"centroid_record_y\", DoubleType(), True),\r\n      StructField(\"classification_extendedness\", DoubleType(), True),\r\n      StructField(\"aperturecorrection\", DoubleType(), True),\r\n      StructField(\"aperturecorrection_err\", DoubleType(), True),\r\n      StructField(\"refFlux\", DoubleType(), True),\r\n      StructField(\"refFlux_err\", DoubleType(), True),\r\n      StructField(\"objectId\", LongType(), True),\r\n      StructField(\"coord_raVar\", DoubleType(), True),\r\n      StructField(\"coord_radeclCov\", DoubleType(), True),\r\n      StructField(\"coord_declVar\", DoubleType(), True),\r\n      StructField(\"exposure_id\", LongType(), True),\r\n      StructField(\"exposure_filter_id\", IntegerType(), True),\r\n      StructField(\"exposure_time\", DoubleType(), True),\r\n      StructField(\"exposure_time_mid\", DoubleType(), True),\r\n      StructField(\"cluster_id\", LongType(), True),\r\n      StructField(\"cluster_coord_ra\", DoubleType(), True),\r\n      StructField(\"cluster_coord_decl\", DoubleType(), True),\r\n\r\n\r\n]) \r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": "# sue: THIS IS THE ONE THAT WORKS\r\n# jvv: Meet's schema, tweaked for with UC's sql<->parquet data type mappings for stuff like INTs and BITs\r\n# except SQL BITs as parquet int32s\r\nfrom pyspark.sql.types import *\r\n\r\n# Source:\r\ncustomSchemaint= StructType([ \r\n      StructField(\"id\", LongType(), True),\r\n      StructField(\"chunkid\", IntegerType(), True),\r\n      StructField(\"coord_ra\", DoubleType(), True),\r\n      StructField(\"coord_decl\", DoubleType(), True),\r\n      StructField(\"coord_htmId20\", LongType(), True),\r\n      StructField(\"parent\", LongType(), True),\r\n      StructField(\"flags_badcentroid\", IntegerType(), True),\r\n      StructField(\"centroid_sdss_x\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_y\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_xVar\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_xyCov\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_yVar\", DoubleType(), True),\r\n      StructField(\"centroid_sdss_flags\", IntegerType(), True),\r\n      StructField(\"flags_pixel_edge\", IntegerType(), True),\r\n      StructField(\"flags_pixel_interpolated_any\", IntegerType(), True),\r\n      StructField(\"flags_pixel_interpolated_center\", IntegerType(), True),\r\n      StructField(\"flags_pixel_saturated_any\", IntegerType(), True),\r\n      StructField(\"flags_pixel_saturated_center\", IntegerType(), True),\r\n      StructField(\"flags_pixel_cr_any\", IntegerType(), True),\r\n      StructField(\"flags_pixel_cr_center\", IntegerType(), True),\r\n      StructField(\"centroid_naive_x\", DoubleType(), True),\r\n      StructField(\"centroid_naive_y\", DoubleType(), True),\r\n      StructField(\"centroid_naive_xVar\", DoubleType(), True),\r\n      StructField(\"centroid_naive_xyCov\", DoubleType(), True),\r\n      StructField(\"centroid_naive_yVar\", DoubleType(), True),\r\n      StructField(\"centroid_naive_flags\", IntegerType(), True),\r\n      StructField(\"centroid_gaussian_x\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_y\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_xVar\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_xyCov\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_yVar\", DoubleType(), True),\r\n      StructField(\"centroid_gaussian_flags\", IntegerType(), True),\r\n      StructField(\"shape_sdss_Ixx\", DoubleType(), True),\r\n      StructField(\"shape_sdss_Iyy\", DoubleType(), True),\r\n      StructField(\"shape_sdss_Ixy\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxxVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxxIyyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxxIxyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IyyVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IyyIxyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_IxyVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_flags\", IntegerType(), True),\r\n      StructField(\"shape_sdss_centroid_x\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_y\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_xVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_xyCov\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_yVar\", DoubleType(), True),\r\n      StructField(\"shape_sdss_centroid_flags\", IntegerType(), True),\r\n      StructField(\"shape_sdss_flags_unweightedbad\", IntegerType(), True),\r\n      StructField(\"shape_sdss_flags_unweighted\", IntegerType(), True),\r\n      StructField(\"shape_sdss_flags_shift\", IntegerType(), True),\r\n      StructField(\"shape_sdss_flags_maxiter\", IntegerType(), True),\r\n      StructField(\"flux_psf\", DoubleType(), True),\r\n      StructField(\"flux_psf_err\", DoubleType(), True),\r\n      StructField(\"flux_psf_flags\", IntegerType(), True),\r\n      StructField(\"flux_psf_psffactor\", DoubleType(), True),\r\n      StructField(\"flux_psf_flags_psffactor\", IntegerType(), True),\r\n      StructField(\"flux_psf_flags_badcorr\", IntegerType(), True),\r\n      StructField(\"flux_naive\", DoubleType(), True),\r\n      StructField(\"flux_naive_err\", DoubleType(), True),\r\n      StructField(\"flux_naive_flags\", IntegerType(), True),\r\n      StructField(\"flux_gaussian\", DoubleType(), True),\r\n      StructField(\"flux_gaussian_err\", DoubleType(), True),\r\n      StructField(\"flux_gaussian_flags\", IntegerType(), True),\r\n      StructField(\"flux_gaussian_psffactor\", DoubleType(), True),\r\n      StructField(\"flux_gaussian_flags_psffactor\", IntegerType(), True),\r\n      StructField(\"flux_gaussian_flags_badcorr\", IntegerType(), True),\r\n      StructField(\"flux_sinc\", DoubleType(), True),\r\n      StructField(\"flux_sinc_err\", DoubleType(), True),\r\n      StructField(\"flux_sinc_flags\", IntegerType(), True),\r\n      StructField(\"centroid_record_x\", DoubleType(), True),\r\n      StructField(\"centroid_record_y\", DoubleType(), True),\r\n      StructField(\"classification_extendedness\", DoubleType(), True),\r\n      StructField(\"aperturecorrection\", DoubleType(), True),\r\n      StructField(\"aperturecorrection_err\", DoubleType(), True),\r\n      StructField(\"refFlux\", DoubleType(), True),\r\n      StructField(\"refFlux_err\", DoubleType(), True),\r\n      StructField(\"objectId\", LongType(), True),\r\n      StructField(\"coord_raVar\", DoubleType(), True),\r\n      StructField(\"coord_radeclCov\", DoubleType(), True),\r\n      StructField(\"coord_declVar\", DoubleType(), True),\r\n      StructField(\"exposure_id\", LongType(), True),\r\n      StructField(\"exposure_filter_id\", IntegerType(), True),\r\n      StructField(\"exposure_time\", DoubleType(), True),\r\n      StructField(\"exposure_time_mid\", DoubleType(), True),\r\n      StructField(\"cluster_id\", LongType(), True),\r\n      StructField(\"cluster_coord_ra\", DoubleType(), True),\r\n      StructField(\"cluster_coord_decl\", DoubleType(), True),\r\n\r\n\r\n]) \r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": "# sue's test to make parquet files close to 1GB in size\r\ndfsue = spark.read.load('/LSST/csv/test/Source', format=\"csv\", sep=',', schema=customSchemaint)\r\n#dfsue.show(2)",
            "metadata": {},
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": "#df1 = spark.read.load('/LSST/Source/csv/', format=\"csv\", sep=';', schema=customSchema, header=\"true\")\r\n#df1 = spark.read.load('/LSST/Source/csv/Source_8945.csv', format=\"csv\", sep=';' , schema=customSchema)\r\n#df1 = spark.read.load('/LSST/Source/csv/Source_8945.csv', format=\"csv\", sep=';', inferSchema=\"true\", header=\"true\")\r\n#df1 = spark.read.load('/LSST/Source/csv/Source_8945.csv', format=\"csv\", sep=';', schema=customSchema, header=\"true\")\r\n\r\ndf1 = spark.read.load('/LSST/csv/Source', format=\"csv\", sep=',', schema=customSchemaint)\r\n#df1 = spark.read.load('/LSST/sue/csv/Source/Source_9659.csv', format=\"csv\", sep=',', inferSchema=\"true\")\r\n#df1 = spark.read.load('/LSST/sue/csv/Source/Source_9659.csv', format=\"csv\", sep=',', schema=customSchemaint)\r\n#df1 = spark.read.load('/LSST/jvv/Source_9659_id-chunkid-coord_ra.csv', format=\"csv\", sep=',', schema=customSchema)\r\n#df1 = spark.read.load('/LSST/jvv/csv/Source', format=\"csv\", sep=',', schema=customSchema)\r\n#df1 = spark.read.load('/LSST/jvv/test', format=\"csv\", sep=',', schema=customSchema)\r\n\r\n\r\n# Aris sample, not identical to Meet's:\r\n'''\r\nresults = spark.read.option(\"inferSchema\", \"true\").csv('/clickstream_data').toDF(\r\n            \"wcs_click_date_sk\", \"wcs_click_time_sk\", \"wcs_sales_sk\", \"wcs_item_sk\", \"wcs_web_page_sk\", \"wcs_user_sk\"\r\n            )\r\n'''\r\n\r\n#df1.printSchema()\r\ndf1.show(2)",
            "metadata": {},
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "dfsue.coalesce(12).write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"source_test_parquet\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": "#df1.coalesce(350).write.parquet(\"/user/hive/warehouse/source/\", mode='overwrite')\r\n\r\n# Meet above v. Aris sample:\r\n# results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"web_clickstreams\")\r\n# this will write the default number of files (lots of small files)\r\ndf1.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"Source\")\r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": "# attempt to write 6000 parquet files from csv\r\nimport datetime\r\n\r\nbefore = datetime.datetime.now()\r\n# coalesce stuff into 6000 files \r\n\r\ndf1.coalesce(6000).write.parquet(\"/user/hive/warehouse/source_new\", mode='overwrite')\r\n## 15h\r\n\r\nafter = datetime.datetime.now()\r\nprint (after - before )",
            "metadata": {},
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {},
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "code",
            "source": "df1.printSchema()",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": "# Disable saving SUCCESS file\r\nsc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \r\n\r\n# Print the current warehouse directory where the parquet files will be stored\r\nprint(spark.conf.get(\"spark.sql.warehouse.dir\"))\r\n\r\n# Save results as parquet file and create hive table\r\nresults.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"web_clickstreams\")\r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "import datetime\r\n\r\nbefore = datetime.datetime.now()\r\n\r\n\r\n# Execute Spark SQL commands\r\n#sqlDF = spark.sql(\"SELECT * FROM Source LIMIT 100\")\r\n\r\n#sqlDF = spark.sql(\"SELECT * FROM Source LIMIT 100\")\r\n#sqlDF = spark.sql(\"select min(coord_ra) from Source\")\r\n#sqlDF = spark.sql(\"SELECT count(*)  FROM Source where flux_sinc between 1 and 1.1\")\r\nsqlDF = spark.sql(\"select id from source limit 10\")\r\n\r\n#sqlDF = spark.sql(\"select * from sourcesue limit 100\")\r\n\r\n\r\nsqlDF.show()\r\nafter = datetime.datetime.now()\r\nprint (after - before )\r\n\r\n#sqlDF = spark.sql(\"SELECT wcs_user_sk, COUNT(*)\\\r\n#                     FROM web_clickstreams\\\r\n#                    WHERE wcs_user_sk IS NOT NULL\\\r\n#                   GROUP BY wcs_user_sk\\\r\n#                   ORDER BY COUNT(*) DESC LIMIT 100\")\r\n#sqlDF.show()",
            "metadata": {},
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "# sue's attempt to read a dir of parquet files into a dataframe\r\ndf = spark.read.load(\"/user/hive/warehouse/source_new\")\r\n#df = spark.read.load(\"/user/hive/warehouse/sourcesue\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": "df.write.saveAsTable(\"source\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": "import datetime\r\n\r\nbefore = datetime.datetime.now()\r\n# coalesce stuff into 6000 files \r\n\r\ndf.coalesce(6).write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"source_new\")\r\n\r\nafter = datetime.datetime.now()\r\nprint (after - before )\r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": "import datetime\r\n\r\nbefore = datetime.datetime.now()\r\ntime.sleep(1)\r\nafter = datetime.datetime.now()\r\nprint (after - before )",
            "metadata": {},
            "outputs": [],
            "execution_count": 16
        },
        {
            "cell_type": "code",
            "source": "import pyspark\r\n\r\n# start\r\nsc = pyspark.SparkContext()\r\n\r\n#stop\r\nsc.stop()",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# Read the product reviews CSV files into a spark data frame, print schema & top rows\r\nresults = spark.read.option(\"inferSchema\", \"true\").csv('/product_review_data').toDF(\r\n            \"pr_review_sk\", \"pr_review_content\"\r\n            )\r\nresults.printSchema()\r\nresults.show()",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# Save results as parquet file and create hive table\r\nresults.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"product_reviews\")\r\n",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "# Execute Spark SQL commands\r\nsqlDF = spark.sql(\"SELECT pr_review_sk, CHAR_LENGTH(pr_review_content) as len FROM product_reviews LIMIT 100\")\r\nsqlDF.show()",
            "metadata": {},
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "code",
            "source": "spark.sql('drop table Source')",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 10
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "execution_count": 1
        }
    ]
}