{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                " \r\n",
                "\r\n",
                "import subprocess\r\n",
                "\r\n",
                " \r\n",
                "\r\n",
                "# Install TensorFlow\r\n",
                "\r\n",
                "stdout = subprocess.check_output(\r\n",
                "\r\n",
                "    \"pip3 install esutil\",\r\n",
                "\r\n",
                "    stderr=subprocess.STDOUT, shell=True).decode(\"utf-8\")\r\n",
                "\r\n",
                "print(stdout)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "92ed746a-1068-47a4-bb80-b9ee05d677eb"
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "response = subprocess.run(\"pip3 install esutil\", shell=True, encoding=\"utf-8\")\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "edd8203a-d30b-4d25-8613-407b59c550fe"
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                " \r\n",
                "\r\n",
                "import subprocess\r\n",
                "\r\n",
                " \r\n",
                "\r\n",
                "# Install TensorFlow\r\n",
                "\r\n",
                "stdout = subprocess.check_output(\r\n",
                "\r\n",
                "    \"pip3 install astropy-healpix\",\r\n",
                "\r\n",
                "    stderr=subprocess.STDOUT, shell=True).decode(\"utf-8\")\r\n",
                "\r\n",
                "print(stdout)"
            ],
            "metadata": {
                "azdata_cell_guid": "2099959c-9f09-42a1-b50a-575aaf87c0c4"
            },
            "outputs": [
{
    "name": "stdout",
    "text": "Starting Spark application\n",
    "output_type": "stream"
}, {
    "data": {
        "text/plain": "<IPython.core.display.HTML object>",
        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>application_1568670927228_0018</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://172.23.25.58:30443/gateway/default/yarn/proxy/application_1568670927228_0018/\">Link</a></td><td><a target=\"_blank\" href=\"https://172.23.25.58:30443/gateway/default/yarn/container/container_1568670927228_0018_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
    },
    "metadata": {},
    "output_type": "display_data"
}, {
    "name": "stdout",
    "text": "SparkSession available as 'spark'.\n",
    "output_type": "stream"
}, {
    "name": "stdout",
    "text": "Requirement already satisfied: astropy-healpix in /usr/local/lib/python3.5/dist-packages (0.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from astropy-healpix) (1.16.2)\nRequirement already satisfied: astropy in /usr/local/lib/python3.5/dist-packages (from astropy-healpix) (3.2.1)\nWARNING: You are using pip version 19.1.1, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.",
    "output_type": "stream"
}
],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "import sys\r\n",
                "print(sys.version)"
            ],
            "metadata": {
                "azdata_cell_guid": "d5022694-441f-473f-b8b5-0de885b214a6"
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "import astropy\r\n",
                "from astropy_healpix import HEALPix\r\n",
                "from astropy import units as u\r\n",
                "from pyspark.sql.functions import udf\r\n",
                "from pyspark.sql.types import IntegerType\r\n",
                "\r\n",
                "#spark = SparkSession.builder.getOrCreate()\r\n",
                "from pyspark.sql.functions import lit\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "def gethealpix(ra, decl):\r\n",
                "    hp = HEALPix(nside=16, order='nested')\r\n",
                "    return hp.lonlat_to_healpix(ra * u.deg, decl * u.deg)\r\n",
                "\r\n",
                "udf_func = udf(gethealpix, IntegerType())\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "sc.setLogLevel(\"WARN\")\r\n",
                "\r\n",
                "\r\n",
                "i = gethealpix(123, 45)\r\n",
                "print(i)\r\n",
                "\r\n",
                "#Read a file and then write it to the SQL table\r\n",
                "datafile = \"/user/hive/warehouse/source_test_parquet\"\r\n",
                "#datafile = \"/user/hive/warehouse/sourcesue2\"\r\n",
                "old_df = spark.read.format('parquet').load(datafile)\r\n",
                "\r\n",
                "#new_df = old_df.withColumn(\"healpix\", udf_func(old_df.coord_ra, old_df.coord_decl)).collect()\r\n",
                "\r\n",
                "\r\n",
                "#this should work to add a column to the df \r\n",
                "\r\n",
                "#new_df.show(1)  #THIS DOES NOT WORK!!!!!!!!!!!!!!!\r\n",
                "#new_df.printSchema()"
            ],
            "metadata": {
                "azdata_cell_guid": "d37f7c6d-36cf-45e5-9422-0f135637880b"
            },
            "outputs": [],
            "execution_count": 26
        },
        {
            "cell_type": "code",
            "source": [
                "import astropy\r\n",
                "from astropy_healpix import HEALPix\r\n",
                "from astropy import units as u"
            ],
            "metadata": {
                "azdata_cell_guid": "fbd3d6ae-cec6-4ead-b394-6b2fd2fdd2f0"
            },
            "outputs": [],
            "execution_count": 15
        },
        {
            "cell_type": "code",
            "source": [
                "hp = HEALPix(nside=16, order='nested')\r\n",
                "\r\n",
                "index = hp.lonlat_to_healpix([1, 3, 4] * u.deg, [5, 6, 9] * u.deg, return_offsets=False)\r\n",
                "index\r\n",
                "\r\n",
                "\r\n",
                "ra = 123.0 * u.deg\r\n",
                "decl = 40.0 * u.deg\r\n",
                "i = hp.lonlat_to_healpix(ra, decl)\r\n",
                "i"
            ],
            "metadata": {
                "azdata_cell_guid": "0a7c7725-9329-4e61-ae0f-2be23440bfb7"
            },
            "outputs": [],
            "execution_count": 16
        },
        {
            "cell_type": "code",
            "source": [
                "#new_df.printSchema()\r\n",
                "#type(new_df)\r\n",
                "new_df.show(1)\r\n",
                "new_df.count()"
            ],
            "metadata": {
                "azdata_cell_guid": "f0c72359-6b8f-4fa9-995f-2285796b7a25"
            },
            "outputs": [],
            "execution_count": 18
        },
        {
            "cell_type": "code",
            "source": [
                "from astropy.coordinates import CartesianRepresentation as Cartesian\r\n",
                "from pyspark.sql.types import FloatType\r\n",
                "\r\n",
                "\r\n",
                "p = Cartesian(1,2,4)\r\n",
                "print(p.x)\r\n",
                "\r\n",
                "def get_cart(x, y, z):\n",
                "    from astropy.coordinates import CartesianRepresentation as Cartesian\r\n",
                "    return Cartesian(x, y, z).x\r\n",
                "\r\n",
                "\r\n",
                "udf_func = udf(get_cart, FloatType())\r\n",
                "\r\n",
                "#nd = old_df.withColumn(\"blah\", Cartesian(old_df.coord_ra, old_df.coord_decl, old_df.coord_decl).x)\r\n",
                "nd = old_df.withColumn(\"blah\", udf_func(old_df.coord_ra, old_df.coord_decl, old_df.coord_decl.toPandas().collect()))\r\n",
                "\r\n",
                "nd.show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "d7d43f31-2edd-4929-8fb6-225b914bf242"
            },
            "outputs": [],
            "execution_count": 30
        }
    ]
}