{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala"
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%%configure -f\n",
                "{\"name\": \"remotesparkmagics-sue\", \"executorMemory\": \"12G\", \"numExecutors\": 10, \n",
                " \"executorCores\": 4,\n",
                " \"conf\": {\"spark.jars\": \"/system/jar/simpleHTM.jar,/system/jar/jhealpix.jar\"}}"
            ],
            "metadata": {
                "azdata_cell_guid": "3b0fab99-92e1-411f-91c9-f9cb8fbe2492"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "import simple.HTMindex\n",
                "import healpix.jhu.Healpix\n",
                "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\n",
                "import org.apache.spark.sql.functions.udf\n",
                "import java.util.Calendar"
            ],
            "metadata": {
                "azdata_cell_guid": "82bb7288-09a2-41c6-9be4-9789abaf6fa2"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "object HTMUtils extends Serializable {\n",
                "    var htmindex= new HTMindex() with Serializable\n",
                "    val htmid: (Double, Double) => Long = htmindex.lookupId(_,_)\n",
                "\n",
                "    val htmidUDF=udf(htmid)\n",
                "    \n",
                "}\n",
                "\n",
                "object HEALPixUtils extends Serializable {\n",
                "    var hp = new Healpix() with Serializable\n",
                "    val healpixid: (Double, Double) => Long = hp.ang2pix(_,_)\n",
                "    val healpixidUDF=udf(healpixid)\n",
                "    \n",
                "}"
            ],
            "metadata": {
                "azdata_cell_guid": "7e6fb307-ecc0-4baf-a03e-3d96e70d5df4"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"hello from scala\")"
            ],
            "metadata": {
                "azdata_cell_guid": "c4b63410-2829-494c-80cf-ed293c40b2a5"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# can we get this login thing to work in scala?\n",
                "this was a pain, it would be better if i ACTUALL knew scala i guess"
            ],
            "metadata": {
                "azdata_cell_guid": "e97d85c1-a9fd-4fa9-bf64-8f35717ea36e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "\n",
                "val authDF = spark.read.json(\"hdfs:///.config/creds.json\")\n",
                "lazy val m = authDF.first.getValuesMap[Any](authDF.schema.fieldNames)\n",
                "\n",
                "lazy val username = (m(\"jdbc_username\"))\n",
                "\n",
                "lazy val password = (m(\"jdbc_password\"))\n",
                "\n",
                "\n",
                "\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6f8666b5-593c-4c56-b345-8f457184afeb"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# get data from parquet files into dataframe, add extra computed columns for htmid and healpixid"
            ],
            "metadata": {
                "azdata_cell_guid": "531bbddd-0aea-4359-a710-90a2c2b4c327"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "val datafile = \"/user/hive/warehouse/object\"\n",
                "val objDF = spark.read.parquet(datafile)\n",
                "\n",
                "val newDF = objDF.withColumn(\"htmid\",HTMUtils.htmidUDF(objDF(\"ra\"),objDF(\"decl\"))).withColumn(\"healpixid\", HEALPixUtils.healpixidUDF(objDF(\"ra\"), objDF(\"decl\")))\n",
                "\n",
                "newDF.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"object_newcols\")\n",
                "\n",
                "\n"
            ],
            "metadata": {
                "azdata_cell_guid": "844e739e-6cae-4368-acaf-f6576c8259c4"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "azdata_cell_guid": "37b769fd-5ad5-4236-8d55-ce044eed8615"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "azdata_cell_guid": "161fa9a7-73b2-4c35-810c-4e28665082a0"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}