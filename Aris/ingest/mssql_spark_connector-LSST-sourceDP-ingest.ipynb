{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# Read CSV into a data frame\r\nIn this step we read the CSV into a data frame and do some basic cleanup steps. \r\n\r\n\r\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "#spark = SparkSession.builder.getOrCreate()\r\nsc.setLogLevel(\"WARN\")\r\n\r\n#Read a file and then write it to the SQL table\r\ndatafile = \"/user/hive/warehouse/source\"\r\ndf = spark.read.format('parquet').load(datafile)\r\ndf.show(1)\r\ndf.printSchema()",
            "metadata": {},
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": "# (PART 2) Write and READ to Data Pool external Tables in Big Data Cluster\r\n- Write dataframe to SQL external table in Data Pools in Big Data Cluste\r\n- Read SQL external Table to Spark dataframe",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "#Write from Spark to SQL table using MSSQL Spark Connector\r\nprint(\"Use MSSQL connector to write to master SQL instance \")\r\n\r\nservername = \"jdbc:sqlserver://master-0.master-svc\"\r\ndbname = \"LSST\"\r\nurl = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n\r\ndbtable = \"SourceDP\"\r\nuser = \"sa\"\r\npassword = \"REDACTED\" # Please specify password here\r\n\r\ndatapool_table = \"SourceDP\"\r\ndatasource_name = \"SqlDataPool\"\r\n\r\nbatchsize = 1000000\r\n\r\n\r\ntry:\r\n  df.write \\\r\n    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n    .mode(\"overwrite\") \\\r\n    .option(\"url\", url) \\\r\n    .option(\"dbtable\", datapool_table) \\\r\n    .option(\"user\", user) \\\r\n    .option(\"password\", password) \\\r\n    .option(\"dataPoolDataSource\",datasource_name)\\\r\n    .option(\"batchsize\",batchsize)\\\r\n    .save()\r\nexcept ValueError as error :\r\n    print(\"MSSQL Connector write failed\", error)\r\n\r\nprint(\"MSSQL Connector write(overwrite) to data pool external table succeeded\")\r\n",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Use MSSQL connector to write to master SQL instance \nMSSQL Connector write(overwrite) to data pool external table succeeded"
                }
            ],
            "execution_count": 4
        }
    ]
}