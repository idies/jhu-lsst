{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Read CSV into a data frame\r\n",
                "In this step we read the CSV into a data frame and do some basic cleanup steps. \r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "%%configure -f\r\n",
                "{\"executorMemory\": \"58g\", \"executorCores\": 30, \"numExecutors\":4}\r\n",
                ""
            ],
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "Current session configs: <tt>{'executorMemory': '58g', 'executorCores': 30, 'numExecutors': 4, 'kind': 'pyspark3'}</tt><br>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1564678375152_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://172.23.25.61:30443/gateway/default/yarn/proxy/application_1564678375152_0001/\">Link</a></td><td><a target=\"_blank\" href=\"https://172.23.25.61:30443/gateway/default/yarn/container/container_1564678375152_0001_01_000001/root\">Link</a></td><td></td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "\r\n",
                "#spark = SparkSession.builder.getOrCreate()\r\n",
                "from pyspark.sql.functions import lit\r\n",
                "\r\n",
                "sc.setLogLevel(\"WARN\")\r\n",
                "\r\n",
                "#Read a file and then write it to the SQL table\r\n",
                "datafile = \"/user/hive/warehouse/source/source_new\"\r\n",
                "#datafile = \"/user/hive/warehouse/sourcesue2\"\r\n",
                "df = spark.read.format('parquet').load(datafile)\r\n",
                "#df.insert(len(df.columns), 'batch_id', 0, allow_duplicates=True)\r\n",
                "\r\n",
                "#this should work to add a column to the df \r\n",
                "df = df.withColumn('batch_id', lit(0))\r\n",
                "df.show(1)\r\n",
                "df.printSchema()"
            ],
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1564678375152_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://filedb14,31433:30433/gateway/default/yarn/proxy/application_1564678375152_0002/\">Link</a></td><td><a target=\"_blank\" href=\"https://filedb14,31433:30433/gateway/default/yarn/container/container_1564678375152_0002_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "+----------------+-------+------------------+------------------+--------------+------+-----------------+-----------------+------------------+------------------+-------------------+-------------------+-------------------+----------------+----------------------------+-------------------------------+-------------------------+----------------------------+------------------+---------------------+-----------------+-----------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+----------------------+-----------------------+----------------------+-----------------------+------------------+------------------+-------------------+-----------------+--------------------+--------------------+------------------+--------------------+------------------+----------------+---------------------+---------------------+------------------------+-------------------------+------------------------+-------------------------+------------------------------+---------------------------+----------------------+------------------------+------------------+-----------------+--------------+------------------+------------------------+----------------------+-----------------+------------------+----------------+------------------+-----------------+-------------------+-----------------------+-----------------------------+---------------------------+-----------------+------------------+---------------+-----------------+-----------------+---------------------------+------------------+----------------------+-----------------+------------------+----------------+------------------+--------------------+--------------------+-----------+------------------+-------------+-----------------+----------+------------------+------------------+--------+\n|              id|chunkid|          coord_ra|        coord_decl| coord_htmId20|parent|flags_badcentroid|  centroid_sdss_x|   centroid_sdss_y|centroid_sdss_xVar|centroid_sdss_xyCov| centroid_sdss_yVar|centroid_sdss_flags|flags_pixel_edge|flags_pixel_interpolated_any|flags_pixel_interpolated_center|flags_pixel_saturated_any|flags_pixel_saturated_center|flags_pixel_cr_any|flags_pixel_cr_center| centroid_naive_x| centroid_naive_y|centroid_naive_xVar|centroid_naive_xyCov|centroid_naive_yVar|centroid_naive_flags|centroid_gaussian_x|centroid_gaussian_y|centroid_gaussian_xVar|centroid_gaussian_xyCov|centroid_gaussian_yVar|centroid_gaussian_flags|    shape_sdss_Ixx|    shape_sdss_Iyy|     shape_sdss_Ixy|shape_sdss_IxxVar|shape_sdss_IxxIyyCov|shape_sdss_IxxIxyCov| shape_sdss_IyyVar|shape_sdss_IyyIxyCov| shape_sdss_IxyVar|shape_sdss_flags|shape_sdss_centroid_x|shape_sdss_centroid_y|shape_sdss_centroid_xVar|shape_sdss_centroid_xyCov|shape_sdss_centroid_yVar|shape_sdss_centroid_flags|shape_sdss_flags_unweightedbad|shape_sdss_flags_unweighted|shape_sdss_flags_shift|shape_sdss_flags_maxiter|          flux_psf|     flux_psf_err|flux_psf_flags|flux_psf_psffactor|flux_psf_flags_psffactor|flux_psf_flags_badcorr|       flux_naive|    flux_naive_err|flux_naive_flags|     flux_gaussian|flux_gaussian_err|flux_gaussian_flags|flux_gaussian_psffactor|flux_gaussian_flags_psffactor|flux_gaussian_flags_badcorr|        flux_sinc|     flux_sinc_err|flux_sinc_flags|centroid_record_x|centroid_record_y|classification_extendedness|aperturecorrection|aperturecorrection_err|          refFlux|       refFlux_err|        objectId|       coord_raVar|     coord_radeclCov|       coord_declVar|exposure_id|exposure_filter_id|exposure_time|exposure_time_mid|cluster_id|  cluster_coord_ra|cluster_coord_decl|batch_id|\n+----------------+-------+------------------+------------------+--------------+------+-----------------+-----------------+------------------+------------------+-------------------+-------------------+-------------------+----------------+----------------------------+-------------------------------+-------------------------+----------------------------+------------------+---------------------+-----------------+-----------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+----------------------+-----------------------+----------------------+-----------------------+------------------+------------------+-------------------+-----------------+--------------------+--------------------+------------------+--------------------+------------------+----------------+---------------------+---------------------+------------------------+-------------------------+------------------------+-------------------------+------------------------------+---------------------------+----------------------+------------------------+------------------+-----------------+--------------+------------------+------------------------+----------------------+-----------------+------------------+----------------+------------------+-----------------+-------------------+-----------------------+-----------------------------+---------------------------+-----------------+------------------+---------------+-----------------+-----------------+---------------------------+------------------+----------------------+-----------------+------------------+----------------+------------------+--------------------+--------------------+-----------+------------------+-------------+-----------------+----------+------------------+------------------+--------+\n|2603832513169779|      0|113.70775326455224|-88.30991901979571|12649452511182|  null|                0|697.3951937849646|285.02113886805523|0.3197639832559531|               null|0.14561034340951906|                  0|               0|                           0|                              0|                        0|                           0|                 0|                    0|697.0319647569927|284.2108417720661|               null|                null|               null|                   0|  697.2998807738778| 285.02464152021344|                  null|                   null|                  null|                      0|1.8563502304162247|1.4501290162785956|0.16491253303414147|1.193707734796553|                null|                null|0.4709560015082866|                null|0.7284361523300709|               0|    697.4893424661686|    284.7936915302488|                    null|                     null|                    null|                        0|                             0|                          0|                     0|                       0|243.04996311917049|51.14994221005294|             0|           8.43838|                       0|                     0|38.91283255815506|134.79111429579854|               0|286.50571835890065|84.31266963076973|                  0|                7.31454|                            0|                          0|61.53878739336688|135.04412026804758|              0|697.6582043881489|284.5778786394853|                        1.0|0.9979635586693848|  4.112252465384053E-4|83.07929132747398|1.8761453355929765|2603832513138027|0.0228337318879116|-1.44991676154470...|0.050151868063696925| 6314230266|                 2|      53.9075|53989.28057714445|      null|113.70775326455224|-88.30991901979571|       0|\n+----------------+-------+------------------+------------------+--------------+------+-----------------+-----------------+------------------+------------------+-------------------+-------------------+-------------------+----------------+----------------------------+-------------------------------+-------------------------+----------------------------+------------------+---------------------+-----------------+-----------------+-------------------+--------------------+-------------------+--------------------+-------------------+-------------------+----------------------+-----------------------+----------------------+-----------------------+------------------+------------------+-------------------+-----------------+--------------------+--------------------+------------------+--------------------+------------------+----------------+---------------------+---------------------+------------------------+-------------------------+------------------------+-------------------------+------------------------------+---------------------------+----------------------+------------------------+------------------+-----------------+--------------+------------------+------------------------+----------------------+-----------------+------------------+----------------+------------------+-----------------+-------------------+-----------------------+-----------------------------+---------------------------+-----------------+------------------+---------------+-----------------+-----------------+---------------------------+------------------+----------------------+-----------------+------------------+----------------+------------------+--------------------+--------------------+-----------+------------------+-------------+-----------------+----------+------------------+------------------+--------+\nonly showing top 1 row\n\nroot\n |-- id: long (nullable = true)\n |-- chunkid: integer (nullable = true)\n |-- coord_ra: double (nullable = true)\n |-- coord_decl: double (nullable = true)\n |-- coord_htmId20: long (nullable = true)\n |-- parent: long (nullable = true)\n |-- flags_badcentroid: integer (nullable = true)\n |-- centroid_sdss_x: double (nullable = true)\n |-- centroid_sdss_y: double (nullable = true)\n |-- centroid_sdss_xVar: double (nullable = true)\n |-- centroid_sdss_xyCov: double (nullable = true)\n |-- centroid_sdss_yVar: double (nullable = true)\n |-- centroid_sdss_flags: integer (nullable = true)\n |-- flags_pixel_edge: integer (nullable = true)\n |-- flags_pixel_interpolated_any: integer (nullable = true)\n |-- flags_pixel_interpolated_center: integer (nullable = true)\n |-- flags_pixel_saturated_any: integer (nullable = true)\n |-- flags_pixel_saturated_center: integer (nullable = true)\n |-- flags_pixel_cr_any: integer (nullable = true)\n |-- flags_pixel_cr_center: integer (nullable = true)\n |-- centroid_naive_x: double (nullable = true)\n |-- centroid_naive_y: double (nullable = true)\n |-- centroid_naive_xVar: double (nullable = true)\n |-- centroid_naive_xyCov: double (nullable = true)\n |-- centroid_naive_yVar: double (nullable = true)\n |-- centroid_naive_flags: integer (nullable = true)\n |-- centroid_gaussian_x: double (nullable = true)\n |-- centroid_gaussian_y: double (nullable = true)\n |-- centroid_gaussian_xVar: double (nullable = true)\n |-- centroid_gaussian_xyCov: double (nullable = true)\n |-- centroid_gaussian_yVar: double (nullable = true)\n |-- centroid_gaussian_flags: integer (nullable = true)\n |-- shape_sdss_Ixx: double (nullable = true)\n |-- shape_sdss_Iyy: double (nullable = true)\n |-- shape_sdss_Ixy: double (nullable = true)\n |-- shape_sdss_IxxVar: double (nullable = true)\n |-- shape_sdss_IxxIyyCov: double (nullable = true)\n |-- shape_sdss_IxxIxyCov: double (nullable = true)\n |-- shape_sdss_IyyVar: double (nullable = true)\n |-- shape_sdss_IyyIxyCov: double (nullable = true)\n |-- shape_sdss_IxyVar: double (nullable = true)\n |-- shape_sdss_flags: integer (nullable = true)\n |-- shape_sdss_centroid_x: double (nullable = true)\n |-- shape_sdss_centroid_y: double (nullable = true)\n |-- shape_sdss_centroid_xVar: double (nullable = true)\n |-- shape_sdss_centroid_xyCov: double (nullable = true)\n |-- shape_sdss_centroid_yVar: double (nullable = true)\n |-- shape_sdss_centroid_flags: integer (nullable = true)\n |-- shape_sdss_flags_unweightedbad: integer (nullable = true)\n |-- shape_sdss_flags_unweighted: integer (nullable = true)\n |-- shape_sdss_flags_shift: integer (nullable = true)\n |-- shape_sdss_flags_maxiter: integer (nullable = true)\n |-- flux_psf: double (nullable = true)\n |-- flux_psf_err: double (nullable = true)\n |-- flux_psf_flags: integer (nullable = true)\n |-- flux_psf_psffactor: double (nullable = true)\n |-- flux_psf_flags_psffactor: integer (nullable = true)\n |-- flux_psf_flags_badcorr: integer (nullable = true)\n |-- flux_naive: double (nullable = true)\n |-- flux_naive_err: double (nullable = true)\n |-- flux_naive_flags: integer (nullable = true)\n |-- flux_gaussian: double (nullable = true)\n |-- flux_gaussian_err: double (nullable = true)\n |-- flux_gaussian_flags: integer (nullable = true)\n |-- flux_gaussian_psffactor: double (nullable = true)\n |-- flux_gaussian_flags_psffactor: integer (nullable = true)\n |-- flux_gaussian_flags_badcorr: integer (nullable = true)\n |-- flux_sinc: double (nullable = true)\n |-- flux_sinc_err: double (nullable = true)\n |-- flux_sinc_flags: integer (nullable = true)\n |-- centroid_record_x: double (nullable = true)\n |-- centroid_record_y: double (nullable = true)\n |-- classification_extendedness: double (nullable = true)\n |-- aperturecorrection: double (nullable = true)\n |-- aperturecorrection_err: double (nullable = true)\n |-- refFlux: double (nullable = true)\n |-- refFlux_err: double (nullable = true)\n |-- objectId: long (nullable = true)\n |-- coord_raVar: double (nullable = true)\n |-- coord_radeclCov: double (nullable = true)\n |-- coord_declVar: double (nullable = true)\n |-- exposure_id: long (nullable = true)\n |-- exposure_filter_id: integer (nullable = true)\n |-- exposure_time: double (nullable = true)\n |-- exposure_time_mid: double (nullable = true)\n |-- cluster_id: long (nullable = true)\n |-- cluster_coord_ra: double (nullable = true)\n |-- cluster_coord_decl: double (nullable = true)\n |-- batch_id: integer (nullable = false)",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "# (PART 2) Write and READ to Data Pool external Tables in Big Data Cluster\r\n",
                "- Write dataframe to SQL external table in Data Pools in Big Data Cluste\r\n",
                "- Read SQL external Table to Spark dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import datetime\r\n",
                "\r\n",
                "before = datetime.datetime.now()\r\n",
                "\r\n",
                "#Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "print(\"Use MSSQL connector to write to master SQL instance \")\r\n",
                "\r\n",
                "servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
                "dbname = \"LSST\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "\r\n",
                "dbtable = \"SourceDP\"\r\n",
                "user = \"sa\"\r\n",
                "password = \"fooRiuzg54\" # Please specify password here\r\n",
                "\r\n",
                "datapool_table = \"SourceDP\"\r\n",
                "datasource_name = \"SqlDataPool\"\r\n",
                "\r\n",
                "batchsize = 1000000\r\n",
                "\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"overwrite\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", datapool_table) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"dataPoolDataSource\",datasource_name)\\\r\n",
                "    .option(\"batchsize\",batchsize)\\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(overwrite) to data pool external table succeeded\")\r\n",
                "\r\n",
                "after = datetime.datetime.now()\r\n",
                "print (after - before )\r\n",
                ""
            ],
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "text": "An error occurred while calling o81.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14563 in stage 2.0 failed 4 times, most recent failure: Lost task 14563.3 in stage 2.0 (TID 20906, storage-0-1.storage-0-svc.filedb.svc.cluster.local, executor 3): com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host data-0-3.data-0-svc, port 1433 has failed. Error: \"Connection refused (Connection refused). Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:226)\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.ConvertConnectExceptionToSQLServerException(SQLServerException.java:277)\n\tat com.microsoft.sqlserver.jdbc.SocketFinder.findSocket(IOBuffer.java:2379)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.open(IOBuffer.java:652)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2379)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n\tat com.microsoft.sqlserver.jdbc.spark.BulkCopyUtils$.savePartition(BulkCopyUtils.scala:39)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:123)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:117)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$.saveTableDataPool(SaveToDataPools.scala:127)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$.saveToDataPools(SaveToDataPools.scala:85)\n\tat com.microsoft.sqlserver.jdbc.spark.DefaultSource.createRelation(DefaultSource.scala:61)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host data-0-3.data-0-svc, port 1433 has failed. Error: \"Connection refused (Connection refused). Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:226)\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.ConvertConnectExceptionToSQLServerException(SQLServerException.java:277)\n\tat com.microsoft.sqlserver.jdbc.SocketFinder.findSocket(IOBuffer.java:2379)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.open(IOBuffer.java:652)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2379)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n\tat com.microsoft.sqlserver.jdbc.spark.BulkCopyUtils$.savePartition(BulkCopyUtils.scala:39)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:123)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:117)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1564678375152_0002/container_1564678375152_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 734, in save\n    self._jwrite.save()\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1564678375152_0002/container_1564678375152_0002_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1564678375152_0002/container_1564678375152_0002_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/tmp/nm-local-dir/usercache/root/appcache/application_1564678375152_0002/container_1564678375152_0002_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o81.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14563 in stage 2.0 failed 4 times, most recent failure: Lost task 14563.3 in stage 2.0 (TID 20906, storage-0-1.storage-0-svc.filedb.svc.cluster.local, executor 3): com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host data-0-3.data-0-svc, port 1433 has failed. Error: \"Connection refused (Connection refused). Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:226)\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.ConvertConnectExceptionToSQLServerException(SQLServerException.java:277)\n\tat com.microsoft.sqlserver.jdbc.SocketFinder.findSocket(IOBuffer.java:2379)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.open(IOBuffer.java:652)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2379)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n\tat com.microsoft.sqlserver.jdbc.spark.BulkCopyUtils$.savePartition(BulkCopyUtils.scala:39)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:123)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:117)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$.saveTableDataPool(SaveToDataPools.scala:127)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$.saveToDataPools(SaveToDataPools.scala:85)\n\tat com.microsoft.sqlserver.jdbc.spark.DefaultSource.createRelation(DefaultSource.scala:61)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host data-0-3.data-0-svc, port 1433 has failed. Error: \"Connection refused (Connection refused). Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:226)\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.ConvertConnectExceptionToSQLServerException(SQLServerException.java:277)\n\tat com.microsoft.sqlserver.jdbc.SocketFinder.findSocket(IOBuffer.java:2379)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.open(IOBuffer.java:652)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2379)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n\tat com.microsoft.sqlserver.jdbc.spark.BulkCopyUtils$.savePartition(BulkCopyUtils.scala:39)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:123)\n\tat com.microsoft.sqlserver.jdbc.spark.SaveToDataPools$$anonfun$1.apply(SaveToDataPools.scala:117)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "\r\n",
                "#testing on small batch of parquet files\r\n",
                "#Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "print(\"Use MSSQL connector to write to master SQL instance \")\r\n",
                "\r\n",
                "servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
                "dbname = \"LSST\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "\r\n",
                "dbtable= \"Source_test\"\r\n",
                "#dbtable = \"SourceDP\"\r\n",
                "user = \"sa\"\r\n",
                "password = \"fooRiuzg54\" # Please specify password here\r\n",
                "\r\n",
                "datapool_table = \"Source_test\"\r\n",
                "datasource_name = \"SqlDataPool\"\r\n",
                "\r\n",
                "batchsize = 1000000\r\n",
                "\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"overwrite\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", datapool_table) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"dataPoolDataSource\",datasource_name)\\\r\n",
                "    .option(\"batchsize\",batchsize)\\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(overwrite) to data pool external table succeeded\")"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 8
        }
    ]
}