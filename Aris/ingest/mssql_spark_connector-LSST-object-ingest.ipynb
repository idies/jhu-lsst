{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Read CSV into a data frame\r\n",
                "In this step we read the CSV into a data frame and do some basic cleanup steps. \r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#spark = SparkSession.builder.getOrCreate()\r\n",
                "sc.setLogLevel(\"WARN\")\r\n",
                "\r\n",
                "#Read a file and then write it to the SQL table\r\n",
                "datafile = \"/user/hive/warehouse/object\"\r\n",
                "df = spark.read.format('parquet').load(datafile)\r\n",
                "df.show(1)\r\n",
                "df.printSchema()"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "# (PART 2) Write and READ to Data Pool external Tables in Big Data Cluster\r\n",
                "- Write dataframe to SQL external table in Data Pools in Big Data Cluste\r\n",
                "- Read SQL external Table to Spark dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "print(\"Use MSSQL connector to write to master SQL instance \")\r\n",
                "\r\n",
                "servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
                "dbname = \"LSST\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "\r\n",
                "dbtable = \"Object\"\r\n",
                "user = \"sa\"\r\n",
                "password = \"REDACTED\" # Please specify password here\r\n",
                "\r\n",
                "datapool_table = \"Object\"\r\n",
                "datasource_name = \"SqlDataPool\"\r\n",
                "\r\n",
                "batchsize = 1000000\r\n",
                "\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"overwrite\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", datapool_table) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"dataPoolDataSource\",datasource_name)\\\r\n",
                "    .option(\"batchsize\",batchsize)\\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(overwrite) to data pool external table succeeded\")\r\n",
                ""
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 4
        }
    ]
}