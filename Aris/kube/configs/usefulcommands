# delete filedb namespace
kubectl delete ns filedb

# prep to hack old Released PVs back to life
kubectl get pv -o yaml `kubectl get pv | grep Releas | awk '{print $1}'` > pvs.released.yaml

# then remind yourself what changes to make between dist and ".tweaked":
filedb14:/<8>kube/configs:4638# cd ctp3.0/
filedb14:/<9>configs/ctp3.0:4639# diff pvs.released.yaml pvs.released.tweaked.yaml|head
26,27d25
<       resourceVersion: "57514288"
<       uid: c22df860-5b9c-11e9-a147-0cc47ad46d50
66,67d63
<       resourceVersion: "57514701"
<       uid: f6dcce8e-5b9c-11e9-a147-0cc47ad46d50
105,106d100
<       resourceVersion: "57515894"
<       uid: 9485bc6e-5b9d-11e9-a147-0cc47ad46d50
145,146d138kubectl

# then go:
>
# scrape configs for "important" statefulsets in prep for "IDIESifying" storage claims
kubectl get -n filedb statefulset.apps/nmnode-0 -o yaml --export > ss.nmnode-0.export.yaml
kubectl get -n filedb statefulset.apps/data-0 -o yaml --export > ss.data-0.export.yaml
kubectl get -n filedb statefulset.apps/storage-0 -o yaml --export > ss.storage-0.export.yaml
kubectl get -n filedb statefulset.apps/master -o yaml --export > ss.master.export.yaml

# copy "export" yamls to "idies" versions and bend to our will

# replace important dist statefulsets with IDIESified ones:
# ONLY DO THIS WITH THE MASTER DONT DO IT WITH NMNODE OR STORAGE!!
kubectl replace -n filedb -f ss.master.idies.yaml --force


# to goof off inside of a container
# this doesn't work because master doesn't have a hadoop container
kubectl exec -n filedb -it master-0 -c hadoop -- bash

# HDFS:
hdfs dfs -ls /
hdfs dfs -du -s /\*
hdfs dfsadmin -report

# copy from local node filesystem into HDFS:
kubectl exec -n filedb -ti storage-0-3 -c hadoop -- bash
hdfs dfs -put /srv/data02/sql_db/sue/csv_exports/vdh1/Object/* /tmp/Objecttest

# kubectl exec screws up your terminal and makes lines wrap around sometimes.
# add this "env TERM=xterm LINES=$LINES COLUMNS=$COLUMNS" stuff, and it will not get messed up
kubectl exec -n filedb -ti storage-0-3 -c hadoop -- env TERM=xterm LINES=$LINES COLUMNS=$COLUMNS /bin/bash

kubectl exec -n filedb -ti nmnode-0-0 -c hadoop -- env TERM=xterm LINES=$LINES COLUMNS=$COLUMNS /bin/bash

kubectl -n filedb delete pod/storage-0-0



# white-knuckle HDFS-preservation hail-mary switcheroo:
kubectl delete -f ss.storage-0.export.yaml
kubectl replace -f ss.nmnode-0.idies.yaml --force

# stuff for Spark ingest
kubectl cp -n filedb ~vincent/proj/sqllinux/aris/git/jhu-lsst/Aris/ingest/spark-ingest-ms-clickstream-example master-0:/tmp -c hadoop
https://filedb14:30443/gateway/default/yarn/cluster/apps
https://filedb14:30443/gateway/default/sparkhistory/

# good shit:
# monitoring!
https://filedb14:30777/grafana

# access other filedb /srv/dataXX volumes from wherever!
# note -- don't forget to zero pad stuff (filedb-01 not filedb-1 etc) 
ls -l /sciserver/filedb01-02

# view / control / kill  yarn applications (spark jobs)
# 1. get onto master node
kubectl exec -n filedb -ti master-0-0 -c hadoop -- env TERM=xterm LINES=$LINES COLUMNS=$COLUMNS /bin/bash
# 2. list yarn apps
yarn application -list
# 3. kill apps with the app id you got from the list command, if you need to
yarn application -kill <app_id>


